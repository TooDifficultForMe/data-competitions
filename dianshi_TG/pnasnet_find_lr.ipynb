{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torchvision import models, transforms\n",
    "import pretrainedmodels\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from data import data_reader\n",
    "\n",
    "Tensor = torch.Tensor\n",
    "rand_m = np.random.random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'pnasnet5large'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class model_pnas(nn.Module):\n",
    "    def __init__(self, num_class):\n",
    "        super(model_pnas, self).__init__()\n",
    "        self.pnas = pretrainedmodels.pnasnet5large(num_classes=1000)\n",
    "        self.argp = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Linear(4320, num_class)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pnas.features(x)\n",
    "        x = self.argp(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import data_reader\n",
    "class args:\n",
    "    bs = 4\n",
    "    path = 'train2000/'\n",
    "\n",
    "p_norm = transforms.Normalize(mean=[0.5, 0.5, 0.5],\n",
    "                             std=[0.5, 0.5, 0.5])\n",
    "p_trm = transforms.Compose([\n",
    "    transforms.Resize(331),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    p_norm\n",
    "])\n",
    "\n",
    "val_p_trm = transforms.Compose([\n",
    "    transforms.Resize(333),\n",
    "    transforms.CenterCrop(331),\n",
    "    transforms.ToTensor(),\n",
    "    p_norm\n",
    "])\n",
    "    \n",
    "    \n",
    "dr = data_reader(args, 'train2000/train2000.csv', trm=val_p_trm, val_trm=val_p_trm)\n",
    "\n",
    "train_loader, test_loader = dr.get_train_loader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.005    0.006875 0.00875  0.010625 0.0125   0.014375 0.01625  0.018125\n",
      " 0.02     0.021875 0.02375  0.025625 0.0275   0.029375 0.03125  0.033125\n",
      " 0.035    0.036875 0.03875  0.040625 0.0425   0.044375 0.04625  0.048125\n",
      " 0.05    ]\n"
     ]
    }
   ],
   "source": [
    "lr_list = np.linspace(0.005, 0.05, 25)\n",
    "print(lr_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Batch # 0 Train Loss 1.75981 \n",
      "Epoch 0 | Batch # 0 Train Loss 1.75981 \n",
      "Epoch 0 | Batch # 1 Train Loss 3.61903 \n",
      "Epoch 0 | Batch # 2 Train Loss 58.78155 \n",
      "Epoch 0 | Batch # 3 Train Loss 17.22742 \n",
      "Epoch 0 | Batch # 4 Train Loss 186.78053 \n",
      "Epoch 0 | Batch # 5 Train Loss 63.64571 \n",
      "Epoch 0 | Batch # 6 Train Loss 53.16903 \n",
      "Epoch 0 | Batch # 7 Train Loss 67.94562 \n",
      "Epoch 0 | Batch # 8 Train Loss 46.63819 \n",
      "Epoch 0 | Batch # 9 Train Loss 33.92497 \n",
      "Epoch 0 | Batch # 10 Train Loss 21.89477 \n",
      "Epoch 0 | Batch # 11 Train Loss 57.46309 \n",
      "Epoch 0 | Batch # 12 Train Loss 30.29081 \n",
      "Epoch 0 | Batch # 13 Train Loss 57.30238 \n",
      "Epoch 0 | Batch # 14 Train Loss 6.19281 \n",
      "Epoch 0 | Batch # 15 Train Loss 54.27122 \n",
      "Epoch 0 | Batch # 16 Train Loss 32.60810 \n",
      "Epoch 0 | Batch # 17 Train Loss 10.05855 \n",
      "Epoch 0 | Batch # 18 Train Loss 15.83625 \n",
      "Epoch 0 | Batch # 19 Train Loss 39.19742 \n",
      "Epoch 0 | Batch # 20 Train Loss 44.71524 \n",
      "Epoch 0 | Batch # 21 Train Loss 13.00549 \n",
      "Epoch 0 | Batch # 22 Train Loss 9.91865 \n",
      "Epoch 0 | Batch # 23 Train Loss 18.52665 \n",
      "Epoch 0 | Batch # 24 Train Loss 43.44773 \n",
      "Epoch 0 | Batch # 25 Train Loss 32.75451 \n",
      "Epoch 0 | Batch # 26 Train Loss 24.62634 \n",
      "Epoch 0 | Batch # 27 Train Loss 8.14500 \n",
      "Epoch 0 | Batch # 28 Train Loss 25.59632 \n",
      "Epoch 0 | Batch # 29 Train Loss 19.15127 \n",
      "Epoch 0 | Batch # 30 Train Loss 8.50223 \n",
      "Epoch 0 | Batch # 31 Train Loss 11.21035 \n",
      "Epoch 0 | Batch # 32 Train Loss 13.24934 \n",
      "Epoch 0 | Batch # 33 Train Loss 0.38752 \n",
      "Epoch 0 | Batch # 34 Train Loss 14.40672 \n",
      "Epoch 0 | Batch # 35 Train Loss 18.20442 \n",
      "Epoch 0 | Batch # 36 Train Loss 3.31149 \n",
      "Epoch 0 | Batch # 37 Train Loss 11.52155 \n",
      "Epoch 0 | Batch # 38 Train Loss 5.87231 \n",
      "Epoch 0 | Batch # 39 Train Loss 3.87042 \n",
      "Epoch 0 | Batch # 40 Train Loss 5.04197 \n",
      "Epoch 0 | Batch # 41 Train Loss 13.55875 \n",
      "Epoch 0 | Batch # 42 Train Loss 4.04247 \n",
      "Epoch 0 | Batch # 43 Train Loss 9.35780 \n",
      "Epoch 0 | Batch # 44 Train Loss 13.63214 \n",
      "Epoch 0 | Batch # 45 Train Loss 15.88911 \n",
      "Epoch 0 | Batch # 46 Train Loss 1.77397 \n",
      "Epoch 0 | Batch # 47 Train Loss 30.01442 \n",
      "Epoch 0 | Batch # 48 Train Loss 29.12059 \n",
      "Epoch 0 | Batch # 49 Train Loss 11.34808 \n",
      "Epoch 0 | Batch # 50 Train Loss 7.33801 \n",
      "Epoch 0 | Batch # 50 Train Loss 7.33801 \n",
      "Epoch 0 | Batch # 51 Train Loss 10.43876 \n",
      "Epoch 0 | Batch # 52 Train Loss 5.47589 \n",
      "Epoch 0 | Batch # 53 Train Loss 7.09054 \n",
      "Epoch 0 | Batch # 54 Train Loss 18.00821 \n",
      "Epoch 0 | Batch # 55 Train Loss 24.43670 \n",
      "Epoch 0 | Batch # 56 Train Loss 14.79277 \n",
      "Epoch 0 | Batch # 57 Train Loss 33.31019 \n",
      "Epoch 0 | Batch # 58 Train Loss 21.65390 \n",
      "Epoch 0 | Batch # 59 Train Loss 11.27436 \n",
      "Epoch 0 | Batch # 60 Train Loss 13.37638 \n",
      "Epoch 0 | Batch # 61 Train Loss 5.44571 \n",
      "Epoch 0 | Batch # 62 Train Loss 10.93143 \n",
      "Epoch 0 | Batch # 63 Train Loss 2.60948 \n",
      "Epoch 0 | Batch # 64 Train Loss 3.14058 \n",
      "Epoch 0 | Batch # 65 Train Loss 1.90237 \n",
      "Epoch 0 | Batch # 66 Train Loss 7.99077 \n",
      "Epoch 0 | Batch # 67 Train Loss 2.71559 \n",
      "Epoch 0 | Batch # 68 Train Loss 10.53425 \n",
      "Epoch 0 | Batch # 69 Train Loss 5.64538 \n",
      "Epoch 0 | Batch # 70 Train Loss 5.92653 \n",
      "Epoch 0 | Batch # 71 Train Loss 10.73038 \n",
      "Epoch 0 | Batch # 72 Train Loss 22.77604 \n",
      "Epoch 0 | Batch # 73 Train Loss 22.62523 \n",
      "Epoch 0 | Batch # 74 Train Loss 5.87420 \n",
      "Epoch 0 | Batch # 75 Train Loss 4.79186 \n",
      "Epoch 0 | Batch # 76 Train Loss 19.23591 \n",
      "Epoch 0 | Batch # 77 Train Loss 17.45838 \n",
      "Epoch 0 | Batch # 78 Train Loss 5.29655 \n",
      "Epoch 0 | Batch # 79 Train Loss 10.66164 \n",
      "Epoch 0 | Batch # 80 Train Loss 15.54885 \n",
      "Epoch 0 | Batch # 81 Train Loss 8.29878 \n",
      "Epoch 0 | Batch # 82 Train Loss 26.52752 \n",
      "Epoch 0 | Batch # 83 Train Loss 1.51508 \n",
      "Epoch 0 | Batch # 84 Train Loss 11.55915 \n",
      "Epoch 0 | Batch # 85 Train Loss 1.32942 \n",
      "Epoch 0 | Batch # 86 Train Loss 10.97104 \n",
      "Epoch 0 | Batch # 87 Train Loss 0.93037 \n",
      "Epoch 0 | Batch # 88 Train Loss 1.71121 \n",
      "Epoch 0 | Batch # 89 Train Loss 0.50969 \n",
      "Epoch 0 | Batch # 90 Train Loss 33.92102 \n",
      "Epoch 0 | Batch # 91 Train Loss 8.25044 \n",
      "Epoch 0 | Batch # 92 Train Loss 14.12808 \n",
      "Epoch 0 | Batch # 93 Train Loss 13.05943 \n",
      "Epoch 0 | Batch # 94 Train Loss 8.44919 \n",
      "Epoch 0 | Batch # 95 Train Loss 2.89822 \n",
      "Epoch 0 | Batch # 96 Train Loss 13.15662 \n",
      "Epoch 0 | Batch # 97 Train Loss 8.65896 \n",
      "Epoch 0 | Batch # 98 Train Loss 15.77233 \n",
      "Epoch 0 | Batch # 99 Train Loss 7.35337 \n",
      "Epoch 0 | Batch # 100 Train Loss 20.04214 \n",
      "Epoch 0 | Batch # 100 Train Loss 20.04214 \n",
      "Epoch 0 | Batch # 101 Train Loss 12.67808 \n",
      "Epoch 0 | Batch # 102 Train Loss 11.67379 \n",
      "Epoch 0 | Batch # 103 Train Loss 27.41452 \n",
      "Epoch 0 | Batch # 104 Train Loss 12.99995 \n",
      "Epoch 0 | Batch # 105 Train Loss 15.47553 \n",
      "Epoch 0 | Batch # 106 Train Loss 5.61009 \n",
      "Epoch 0 | Batch # 107 Train Loss 14.17975 \n",
      "Epoch 0 | Batch # 108 Train Loss 18.48160 \n",
      "Epoch 0 | Batch # 109 Train Loss 9.58546 \n",
      "Epoch 0 | Batch # 110 Train Loss 5.87485 \n",
      "Epoch 0 | Batch # 111 Train Loss 8.13448 \n",
      "Epoch 0 | Batch # 112 Train Loss 10.21411 \n",
      "Epoch 0 | Batch # 113 Train Loss 14.94300 \n",
      "Epoch 0 | Batch # 114 Train Loss 8.14194 \n",
      "Epoch 0 | Batch # 115 Train Loss 5.20331 \n",
      "Epoch 0 | Batch # 116 Train Loss 4.28977 \n",
      "Epoch 0 | Batch # 117 Train Loss 3.91189 \n",
      "Epoch 0 | Batch # 118 Train Loss 3.46722 \n",
      "Epoch 0 | Batch # 119 Train Loss 10.18783 \n",
      "Epoch 0 | Batch # 120 Train Loss 6.73503 \n",
      "Epoch 0 | Batch # 121 Train Loss 2.63769 \n",
      "Epoch 0 | Batch # 122 Train Loss 5.81676 \n",
      "Epoch 0 | Batch # 123 Train Loss 6.62535 \n",
      "Epoch 0 | Batch # 124 Train Loss 5.22076 \n",
      "Epoch 0 | Batch # 125 Train Loss 3.86800 \n",
      "Epoch 0 | Batch # 126 Train Loss 10.89728 \n",
      "Epoch 0 | Batch # 127 Train Loss 11.13470 \n",
      "Epoch 0 | Batch # 128 Train Loss 5.75976 \n",
      "Epoch 0 | Batch # 129 Train Loss 7.06187 \n",
      "Epoch 0 | Batch # 130 Train Loss 8.01942 \n",
      "Epoch 0 | Batch # 131 Train Loss 7.58344 \n",
      "Epoch 0 | Batch # 132 Train Loss 0.17033 \n",
      "Epoch 0 | Batch # 133 Train Loss 4.83736 \n",
      "Epoch 0 | Batch # 134 Train Loss 4.59077 \n",
      "Epoch 0 | Batch # 135 Train Loss 36.86220 \n",
      "Epoch 0 | Batch # 136 Train Loss 34.52618 \n",
      "Epoch 0 | Batch # 137 Train Loss 23.52981 \n",
      "Epoch 0 | Batch # 138 Train Loss 1.50298 \n",
      "Epoch 0 | Batch # 139 Train Loss 7.15846 \n",
      "Epoch 0 | Batch # 140 Train Loss 23.32468 \n",
      "Epoch 0 | Batch # 141 Train Loss 22.86960 \n",
      "Epoch 0 | Batch # 142 Train Loss 16.30523 \n",
      "Epoch 0 | Batch # 143 Train Loss 17.34434 \n",
      "Epoch 0 | Batch # 144 Train Loss 0.60964 \n",
      "Epoch 0 | Batch # 145 Train Loss 12.92399 \n",
      "Epoch 0 | Batch # 146 Train Loss 9.48479 \n",
      "Epoch 0 | Batch # 147 Train Loss 8.21370 \n",
      "Epoch 0 | Batch # 148 Train Loss 1.11016 \n",
      "Epoch 0 | Batch # 149 Train Loss 9.62929 \n",
      "Epoch 0 | Batch # 150 Train Loss 4.14450 \n",
      "Epoch 0 | Batch # 150 Train Loss 4.14450 \n",
      "Epoch 0 | Batch # 151 Train Loss 6.40048 \n",
      "Epoch 0 | Batch # 152 Train Loss 6.41439 \n",
      "Epoch 0 | Batch # 153 Train Loss 4.16588 \n",
      "Epoch 0 | Batch # 154 Train Loss 6.55461 \n",
      "Epoch 0 | Batch # 155 Train Loss 5.30915 \n",
      "Epoch 0 | Batch # 156 Train Loss 4.65553 \n",
      "Epoch 0 | Batch # 157 Train Loss 6.93802 \n",
      "Epoch 0 | Batch # 158 Train Loss 5.48105 \n",
      "Epoch 0 | Batch # 159 Train Loss 0.63720 \n",
      "Epoch 0 | Batch # 160 Train Loss 2.44654 \n",
      "Epoch 0 | Batch # 161 Train Loss 8.12484 \n",
      "Epoch 0 | Batch # 162 Train Loss 0.75098 \n",
      "Epoch 0 | Batch # 163 Train Loss 6.58619 \n",
      "Epoch 0 | Batch # 164 Train Loss 13.63058 \n",
      "Epoch 0 | Batch # 165 Train Loss 9.11549 \n",
      "Epoch 0 | Batch # 166 Train Loss 4.48801 \n",
      "Epoch 0 | Batch # 167 Train Loss 5.45544 \n",
      "Epoch 0 | Batch # 168 Train Loss 4.78309 \n",
      "Epoch 0 | Batch # 169 Train Loss 10.72575 \n",
      "Epoch 0 | Batch # 170 Train Loss 2.30604 \n",
      "Epoch 0 | Batch # 171 Train Loss 4.82630 \n",
      "Epoch 0 | Batch # 172 Train Loss 1.65233 \n",
      "Epoch 0 | Batch # 173 Train Loss 1.40199 \n",
      "Epoch 0 | Batch # 174 Train Loss 4.96812 \n",
      "Epoch 0 | Batch # 175 Train Loss 2.70139 \n",
      "Epoch 0 | Batch # 176 Train Loss 1.61369 \n",
      "Epoch 0 | Batch # 177 Train Loss 1.01312 \n",
      "Epoch 0 | Batch # 178 Train Loss 5.50163 \n",
      "Epoch 0 | Batch # 179 Train Loss 1.59248 \n",
      "Epoch 0 | Batch # 180 Train Loss 2.33797 \n",
      "Epoch 0 | Batch # 181 Train Loss 3.54709 \n",
      "Epoch 0 | Batch # 182 Train Loss 4.82794 \n",
      "Epoch 0 | Batch # 183 Train Loss 4.15463 \n",
      "Epoch 0 | Batch # 184 Train Loss 1.46011 \n",
      "Epoch 0 | Batch # 185 Train Loss 2.65899 \n",
      "Epoch 0 | Batch # 186 Train Loss 4.86712 \n",
      "Epoch 0 | Batch # 187 Train Loss 3.33737 \n",
      "Epoch 0 | Batch # 188 Train Loss 3.59618 \n",
      "Epoch 0 | Batch # 189 Train Loss 4.04604 \n",
      "Epoch 0 | Batch # 190 Train Loss 5.02832 \n",
      "Epoch 0 | Batch # 191 Train Loss 3.17633 \n",
      "Epoch 0 | Batch # 192 Train Loss 1.28444 \n",
      "Epoch 0 | Batch # 193 Train Loss 4.34308 \n",
      "Epoch 0 | Batch # 194 Train Loss 5.47113 \n",
      "Epoch 0 | Batch # 195 Train Loss 6.57339 \n",
      "Epoch 0 | Batch # 196 Train Loss 2.11499 \n",
      "Epoch 0 | Batch # 197 Train Loss 1.87939 \n",
      "Epoch 0 | Batch # 198 Train Loss 7.32986 \n",
      "Epoch 0 | Batch # 199 Train Loss 1.95211 \n",
      "Epoch 0 | Batch # 200 Train Loss 2.56105 \n",
      "Epoch 0 | Batch # 200 Train Loss 2.56105 \n",
      "Epoch 0 | Batch # 201 Train Loss 2.61310 \n",
      "Epoch 0 | Batch # 202 Train Loss 2.54990 \n",
      "Epoch 0 | Batch # 203 Train Loss 5.62995 \n",
      "Epoch 0 | Batch # 204 Train Loss 1.33036 \n",
      "Epoch 0 | Batch # 205 Train Loss 6.70589 \n",
      "Epoch 0 | Batch # 206 Train Loss 1.72685 \n",
      "Epoch 0 | Batch # 207 Train Loss 2.01792 \n",
      "Epoch 0 | Batch # 208 Train Loss 5.23873 \n",
      "Epoch 0 | Batch # 209 Train Loss 0.93380 \n",
      "Epoch 0 | Batch # 210 Train Loss 3.49912 \n",
      "Epoch 0 | Batch # 211 Train Loss 8.17735 \n",
      "Epoch 0 | Batch # 212 Train Loss 1.96011 \n",
      "Epoch 0 | Batch # 213 Train Loss 2.92824 \n",
      "Epoch 0 | Batch # 214 Train Loss 4.80944 \n",
      "Epoch 0 | Batch # 215 Train Loss 1.37603 \n",
      "Epoch 0 | Batch # 216 Train Loss 5.68308 \n",
      "Epoch 0 | Batch # 217 Train Loss 7.95527 \n",
      "Epoch 0 | Batch # 218 Train Loss 4.21080 \n",
      "Epoch 0 | Batch # 219 Train Loss 3.59897 \n",
      "Epoch 0 | Batch # 220 Train Loss 8.02870 \n",
      "Epoch 0 | Batch # 221 Train Loss 7.63809 \n",
      "Epoch 0 | Batch # 222 Train Loss 9.13802 \n",
      "Epoch 0 | Batch # 223 Train Loss 12.04534 \n",
      "Epoch 0 | Batch # 224 Train Loss 13.55258 \n",
      "Epoch 0 | Batch # 225 Train Loss 14.12881 \n",
      "Epoch 0 | Batch # 226 Train Loss 3.43550 \n",
      "Epoch 0 | Batch # 227 Train Loss 2.86850 \n",
      "Epoch 0 | Batch # 228 Train Loss 3.07112 \n",
      "Epoch 0 | Batch # 229 Train Loss 3.07467 \n",
      "Epoch 0 | Batch # 230 Train Loss 2.40793 \n",
      "Epoch 0 | Batch # 231 Train Loss 1.99893 \n",
      "Epoch 0 | Batch # 232 Train Loss 6.75231 \n",
      "Epoch 0 | Batch # 233 Train Loss 13.69479 \n",
      "Epoch 0 | Batch # 234 Train Loss 6.67298 \n",
      "Epoch 0 | Batch # 235 Train Loss 4.93273 \n",
      "Epoch 0 | Batch # 236 Train Loss 8.05277 \n",
      "Epoch 0 | Batch # 237 Train Loss 5.09633 \n",
      "Epoch 0 | Batch # 238 Train Loss 4.08942 \n",
      "Epoch 0 | Batch # 239 Train Loss 5.43206 \n",
      "Epoch 0 | Batch # 240 Train Loss 3.24479 \n",
      "Epoch 0 | Batch # 241 Train Loss 5.03384 \n",
      "Epoch 0 | Batch # 242 Train Loss 10.99328 \n",
      "Epoch 0 | Batch # 243 Train Loss 4.89309 \n",
      "Epoch 0 | Batch # 244 Train Loss 4.45971 \n",
      "Epoch 0 | Batch # 245 Train Loss 5.45750 \n",
      "Epoch 0 | Batch # 246 Train Loss 15.27707 \n",
      "Epoch 0 | Batch # 247 Train Loss 14.60659 \n",
      "Epoch 0 | Batch # 248 Train Loss 12.82267 \n",
      "Epoch 0 | Batch # 249 Train Loss 1.67365 \n",
      "Epoch 0 | Batch # 250 Train Loss 6.18883 \n",
      "Epoch 0 | Batch # 250 Train Loss 6.18883 \n",
      "Epoch 0 | Batch # 251 Train Loss 9.12162 \n",
      "Epoch 0 | Batch # 252 Train Loss 7.05653 \n",
      "Epoch 0 | Batch # 253 Train Loss 8.49703 \n",
      "Epoch 0 | Batch # 254 Train Loss 2.18149 \n",
      "Epoch 0 | Batch # 255 Train Loss 4.96123 \n",
      "Epoch 0 | Batch # 256 Train Loss 5.65838 \n",
      "Epoch 0 | Batch # 257 Train Loss 6.33401 \n",
      "Epoch 0 | Batch # 258 Train Loss 9.86959 \n",
      "Epoch 0 | Batch # 259 Train Loss 7.77415 \n",
      "Epoch 0 | Batch # 260 Train Loss 9.55748 \n",
      "Epoch 0 | Batch # 261 Train Loss 8.66916 \n",
      "Epoch 0 | Batch # 262 Train Loss 2.27724 \n",
      "Epoch 0 | Batch # 263 Train Loss 8.90618 \n",
      "Epoch 0 | Batch # 264 Train Loss 6.38647 \n",
      "Epoch 0 | Batch # 265 Train Loss 5.08257 \n",
      "Epoch 0 | Batch # 266 Train Loss 3.43032 \n",
      "Epoch 0 | Batch # 267 Train Loss 8.75399 \n",
      "Epoch 0 | Batch # 268 Train Loss 6.80255 \n",
      "Epoch 0 | Batch # 269 Train Loss 0.29132 \n",
      "Epoch 0 | Batch # 270 Train Loss 5.32857 \n",
      "Epoch 0 | Batch # 271 Train Loss 2.30408 \n",
      "Epoch 0 | Batch # 272 Train Loss 6.14452 \n",
      "Epoch 0 | Batch # 273 Train Loss 3.27998 \n",
      "Epoch 0 | Batch # 274 Train Loss 6.48348 \n",
      "Epoch 0 | Batch # 275 Train Loss 2.74329 \n",
      "Epoch 0 | Batch # 276 Train Loss 5.03430 \n",
      "Epoch 0 | Batch # 277 Train Loss 3.60912 \n",
      "Epoch 0 | Batch # 278 Train Loss 3.49504 \n",
      "Epoch 0 | Batch # 279 Train Loss 4.65190 \n",
      "Epoch 0 | Batch # 280 Train Loss 4.48955 \n",
      "Epoch 0 | Batch # 281 Train Loss 4.74224 \n",
      "Epoch 0 | Batch # 282 Train Loss 6.70170 \n",
      "Epoch 0 | Batch # 283 Train Loss 5.27766 \n",
      "Epoch 0 | Batch # 284 Train Loss 1.81341 \n",
      "Epoch 0 | Batch # 285 Train Loss 2.51883 \n",
      "Epoch 0 | Batch # 286 Train Loss 3.01188 \n",
      "Epoch 0 | Batch # 287 Train Loss 1.10413 \n",
      "Epoch 0 | Batch # 288 Train Loss 0.63378 \n",
      "Epoch 0 | Batch # 289 Train Loss 6.28234 \n",
      "Epoch 0 | Batch # 290 Train Loss 3.56124 \n",
      "Epoch 0 | Batch # 291 Train Loss 1.74092 \n",
      "Epoch 0 | Batch # 292 Train Loss 3.20617 \n",
      "Epoch 0 | Batch # 293 Train Loss 6.09089 \n",
      "Epoch 0 | Batch # 294 Train Loss 6.30135 \n",
      "Epoch 0 | Batch # 295 Train Loss 3.39811 \n",
      "Epoch 0 | Batch # 296 Train Loss 9.56878 \n",
      "Epoch 0 | Batch # 297 Train Loss 3.38473 \n",
      "Epoch 0 | Batch # 298 Train Loss 2.70336 \n",
      "Epoch 0 | Batch # 299 Train Loss 3.51955 \n",
      "Epoch 0 | Batch # 300 Train Loss 0.88726 \n",
      "Epoch 0 | Batch # 300 Train Loss 0.88726 \n",
      "Epoch 0 | Batch # 301 Train Loss 1.18203 \n",
      "Epoch 0 | Batch # 302 Train Loss 3.14897 \n",
      "Epoch 0 | Batch # 303 Train Loss 1.61887 \n",
      "Epoch 0 | Batch # 304 Train Loss 2.42482 \n",
      "Epoch 0 | Batch # 305 Train Loss 2.44555 \n",
      "Epoch 0 | Batch # 306 Train Loss 4.13308 \n",
      "Epoch 0 | Batch # 307 Train Loss 2.56655 \n",
      "Epoch 0 | Batch # 308 Train Loss 2.38570 \n",
      "Epoch 0 | Batch # 309 Train Loss 2.33603 \n",
      "Epoch 0 | Batch # 310 Train Loss 1.69266 \n",
      "Epoch 0 | Batch # 311 Train Loss 7.70531 \n",
      "Epoch 0 | Batch # 312 Train Loss 8.39951 \n",
      "Epoch 0 | Batch # 313 Train Loss 7.80174 \n",
      "Epoch 0 | Batch # 314 Train Loss 9.34452 \n",
      "Epoch 0 | Batch # 315 Train Loss 7.28405 \n",
      "Epoch 0 | Batch # 316 Train Loss 3.02261 \n",
      "Epoch 0 | Batch # 317 Train Loss 2.83813 \n",
      "Epoch 0 | Batch # 318 Train Loss 2.30170 \n",
      "Epoch 0 | Batch # 319 Train Loss 2.98838 \n",
      "Epoch 0 | Batch # 320 Train Loss 3.58561 \n",
      "Epoch 0 | Batch # 321 Train Loss 4.21648 \n",
      "Epoch 0 | Batch # 322 Train Loss 3.05566 \n",
      "Epoch 0 | Batch # 323 Train Loss 5.02644 \n",
      "Epoch 0 | Batch # 324 Train Loss 0.02386 \n",
      "Epoch 0 | Batch # 325 Train Loss 7.81940 \n",
      "Epoch 0 | Batch # 326 Train Loss 3.43646 \n",
      "Epoch 0 | Batch # 327 Train Loss 5.10946 \n",
      "Epoch 0 | Batch # 328 Train Loss 3.09794 \n",
      "Epoch 0 | Batch # 329 Train Loss 2.87138 \n",
      "Epoch 0 | Batch # 330 Train Loss 2.74669 \n",
      "Epoch 0 | Batch # 331 Train Loss 3.13208 \n",
      "Epoch 0 | Batch # 332 Train Loss 1.79484 \n",
      "Epoch 0 | Batch # 333 Train Loss 4.02517 \n",
      "Epoch 0 | Batch # 334 Train Loss 1.97970 \n",
      "Epoch 0 | Batch # 335 Train Loss 1.67145 \n",
      "Epoch 0 | Batch # 336 Train Loss 1.51418 \n",
      "Epoch 0 | Batch # 337 Train Loss 3.67818 \n",
      "Epoch 0 | Batch # 338 Train Loss 2.93190 \n",
      "Epoch 0 | Batch # 339 Train Loss 3.82462 \n",
      "Epoch 0 | Batch # 340 Train Loss 0.82376 \n",
      "Epoch 0 | Batch # 341 Train Loss 5.11795 \n",
      "Epoch 0 | Batch # 342 Train Loss 2.04689 \n",
      "Epoch 0 | Batch # 343 Train Loss 1.90535 \n",
      "Epoch 0 | Batch # 344 Train Loss 1.11855 \n",
      "Epoch 0 | Batch # 345 Train Loss 2.45190 \n",
      "Epoch 0 | Batch # 346 Train Loss 1.96783 \n",
      "Epoch 0 | Batch # 347 Train Loss 0.92856 \n",
      "Epoch 0 | Batch # 348 Train Loss 3.38391 \n",
      "Epoch 0 | Batch # 349 Train Loss 2.74692 \n",
      "Epoch 0 | Batch # 350 Train Loss 2.24970 \n",
      "Epoch 0 | Batch # 350 Train Loss 2.24970 \n",
      "Epoch 0 | Batch # 351 Train Loss 4.68634 \n",
      "Epoch 0 | Batch # 352 Train Loss 2.34566 \n",
      "Epoch 0 | Batch # 353 Train Loss 4.26942 \n",
      "Epoch 0 | Batch # 354 Train Loss 3.10872 \n",
      "Epoch 0 | Batch # 355 Train Loss 5.56579 \n",
      "Epoch 0 | Batch # 356 Train Loss 3.14771 \n",
      "Epoch 0 | Batch # 357 Train Loss 2.61043 \n",
      "Epoch 0 | Batch # 358 Train Loss 3.15889 \n",
      "Epoch 0 | Batch # 359 Train Loss 2.74598 \n",
      "Epoch 0 | Batch # 360 Train Loss 1.32096 \n",
      "Epoch 0 | Batch # 361 Train Loss 2.88636 \n",
      "Epoch 0 | Batch # 362 Train Loss 4.40913 \n",
      "Epoch 0 | Batch # 363 Train Loss 1.12142 \n",
      "Epoch 0 | Batch # 364 Train Loss 5.75232 \n",
      "Epoch 0 | Batch # 365 Train Loss 6.88813 \n",
      "Epoch 0 | Batch # 366 Train Loss 3.84233 \n",
      "Epoch 0 | Batch # 367 Train Loss 3.83235 \n",
      "Epoch 0 | Batch # 368 Train Loss 4.43788 \n",
      "Epoch 0 | Batch # 369 Train Loss 3.70800 \n",
      "Epoch 0 | Batch # 370 Train Loss 4.43222 \n",
      "Epoch 0 | Batch # 371 Train Loss 1.54660 \n",
      "Epoch 0 | Batch # 372 Train Loss 3.89077 \n",
      "Epoch 0 | Batch # 373 Train Loss 2.12255 \n",
      "Epoch 0 | Batch # 374 Train Loss 1.48953 \n",
      "Epoch 0 | Batch # 375 Train Loss 2.08297 \n",
      "Epoch 0 | Batch # 376 Train Loss 4.78466 \n",
      "Epoch 0 | Batch # 377 Train Loss 3.48658 \n",
      "Epoch 0 | Batch # 378 Train Loss 3.65031 \n",
      "Epoch 0 | Batch # 379 Train Loss 2.08530 \n",
      "Epoch 0 | Batch # 380 Train Loss 2.28070 \n",
      "Epoch 0 | Batch # 381 Train Loss 2.79026 \n",
      "Epoch 0 | Batch # 382 Train Loss 1.14301 \n",
      "Epoch 0 | Batch # 383 Train Loss 1.91524 \n",
      "Epoch 0 | Batch # 384 Train Loss 3.03949 \n",
      "Epoch 0 | Batch # 385 Train Loss 0.89002 \n",
      "Epoch 0 | Batch # 386 Train Loss 0.92811 \n",
      "Epoch 0 | Batch # 387 Train Loss 6.16126 \n",
      "Epoch 0 | Batch # 388 Train Loss 3.83382 \n",
      "Epoch 0 | Batch # 389 Train Loss 1.93709 \n",
      "Epoch 0 | Batch # 390 Train Loss 5.05730 \n",
      "Epoch 0 | Batch # 391 Train Loss 2.28955 \n",
      "Epoch 0 | Batch # 392 Train Loss 1.12147 \n",
      "Epoch 0 | Batch # 393 Train Loss 2.96274 \n",
      "Epoch 0 | Batch # 394 Train Loss 5.40260 \n",
      "Epoch 0 | Batch # 395 Train Loss 1.94707 \n",
      "Epoch 0 | Batch # 396 Train Loss 1.75262 \n",
      "Epoch 0 | Batch # 397 Train Loss 2.82568 \n",
      "Epoch 0 | Batch # 398 Train Loss 2.37995 \n",
      "Epoch 0 | Batch # 399 Train Loss 4.26364 \n",
      "\n",
      "Epoch 0 | Epoch Train Loss 8.72552\n",
      "Epoch 0 | Epoch Train Acc 26.188%\n",
      "Epoch 0 | Epoch Val Loss 5.10144\n",
      "Epoch 0 | Epoch Val Acc 30.500%\n",
      "\n",
      "Epoch 1 | Batch # 0 Train Loss 4.59625 \n",
      "Epoch 1 | Batch # 0 Train Loss 4.59625 \n",
      "Epoch 1 | Batch # 1 Train Loss 3.51540 \n",
      "Epoch 1 | Batch # 2 Train Loss 2.46651 \n",
      "Epoch 1 | Batch # 3 Train Loss 4.66385 \n",
      "Epoch 1 | Batch # 4 Train Loss 3.61359 \n",
      "Epoch 1 | Batch # 5 Train Loss 8.33864 \n",
      "Epoch 1 | Batch # 6 Train Loss 5.61133 \n",
      "Epoch 1 | Batch # 7 Train Loss 3.24202 \n",
      "Epoch 1 | Batch # 8 Train Loss 1.12074 \n",
      "Epoch 1 | Batch # 9 Train Loss 1.69532 \n",
      "Epoch 1 | Batch # 10 Train Loss 2.07335 \n",
      "Epoch 1 | Batch # 11 Train Loss 0.22256 \n",
      "Epoch 1 | Batch # 12 Train Loss 4.07478 \n",
      "Epoch 1 | Batch # 13 Train Loss 9.47047 \n",
      "Epoch 1 | Batch # 14 Train Loss 3.55434 \n",
      "Epoch 1 | Batch # 15 Train Loss 6.82948 \n",
      "Epoch 1 | Batch # 16 Train Loss 0.36707 \n",
      "Epoch 1 | Batch # 17 Train Loss 12.23369 \n",
      "Epoch 1 | Batch # 18 Train Loss 0.22826 \n",
      "Epoch 1 | Batch # 19 Train Loss 8.37110 \n",
      "Epoch 1 | Batch # 20 Train Loss 0.51317 \n",
      "Epoch 1 | Batch # 21 Train Loss 2.08131 \n",
      "Epoch 1 | Batch # 22 Train Loss 2.49107 \n",
      "Epoch 1 | Batch # 23 Train Loss 4.98316 \n",
      "Epoch 1 | Batch # 24 Train Loss 3.10430 \n",
      "Epoch 1 | Batch # 25 Train Loss 1.37638 \n",
      "Epoch 1 | Batch # 26 Train Loss 2.56399 \n",
      "Epoch 1 | Batch # 27 Train Loss 4.34383 \n",
      "Epoch 1 | Batch # 28 Train Loss 1.58252 \n",
      "Epoch 1 | Batch # 29 Train Loss 3.41353 \n",
      "Epoch 1 | Batch # 30 Train Loss 4.00760 \n",
      "Epoch 1 | Batch # 31 Train Loss 1.79970 \n",
      "Epoch 1 | Batch # 32 Train Loss 3.32637 \n",
      "Epoch 1 | Batch # 33 Train Loss 6.10817 \n",
      "Epoch 1 | Batch # 34 Train Loss 4.09413 \n",
      "Epoch 1 | Batch # 35 Train Loss 0.73261 \n",
      "Epoch 1 | Batch # 36 Train Loss 0.33373 \n",
      "Epoch 1 | Batch # 37 Train Loss 6.04070 \n",
      "Epoch 1 | Batch # 38 Train Loss 2.36172 \n",
      "Epoch 1 | Batch # 39 Train Loss 2.14681 \n",
      "Epoch 1 | Batch # 40 Train Loss 7.99316 \n",
      "Epoch 1 | Batch # 41 Train Loss 4.04573 \n",
      "Epoch 1 | Batch # 42 Train Loss 7.06650 \n",
      "Epoch 1 | Batch # 43 Train Loss 4.39748 \n",
      "Epoch 1 | Batch # 44 Train Loss 2.57069 \n",
      "Epoch 1 | Batch # 45 Train Loss 1.45401 \n",
      "Epoch 1 | Batch # 46 Train Loss 1.78150 \n",
      "Epoch 1 | Batch # 47 Train Loss 1.28222 \n",
      "Epoch 1 | Batch # 48 Train Loss 0.71899 \n",
      "Epoch 1 | Batch # 49 Train Loss 4.59975 \n",
      "Epoch 1 | Batch # 50 Train Loss 0.27308 \n",
      "Epoch 1 | Batch # 50 Train Loss 0.27308 \n",
      "Epoch 1 | Batch # 51 Train Loss 4.42128 \n",
      "Epoch 1 | Batch # 52 Train Loss 5.12316 \n",
      "Epoch 1 | Batch # 53 Train Loss 1.83907 \n",
      "Epoch 1 | Batch # 54 Train Loss 2.24586 \n",
      "Epoch 1 | Batch # 55 Train Loss 1.45711 \n",
      "Epoch 1 | Batch # 56 Train Loss 3.05388 \n",
      "Epoch 1 | Batch # 57 Train Loss 3.42171 \n",
      "Epoch 1 | Batch # 58 Train Loss 2.09619 \n",
      "Epoch 1 | Batch # 59 Train Loss 6.54149 \n",
      "Epoch 1 | Batch # 60 Train Loss 2.41210 \n",
      "Epoch 1 | Batch # 61 Train Loss 1.28270 \n",
      "Epoch 1 | Batch # 62 Train Loss 1.61397 \n",
      "Epoch 1 | Batch # 63 Train Loss 1.62574 \n",
      "Epoch 1 | Batch # 64 Train Loss 1.74327 \n",
      "Epoch 1 | Batch # 65 Train Loss 2.94461 \n",
      "Epoch 1 | Batch # 66 Train Loss 2.67549 \n",
      "Epoch 1 | Batch # 67 Train Loss 3.69765 \n",
      "Epoch 1 | Batch # 68 Train Loss 2.04297 \n",
      "Epoch 1 | Batch # 69 Train Loss 4.44061 \n",
      "Epoch 1 | Batch # 70 Train Loss 2.46753 \n",
      "Epoch 1 | Batch # 71 Train Loss 1.54421 \n",
      "Epoch 1 | Batch # 72 Train Loss 1.54237 \n",
      "Epoch 1 | Batch # 73 Train Loss 4.27192 \n",
      "Epoch 1 | Batch # 74 Train Loss 1.61777 \n",
      "Epoch 1 | Batch # 75 Train Loss 1.94444 \n",
      "Epoch 1 | Batch # 76 Train Loss 1.76986 \n",
      "Epoch 1 | Batch # 77 Train Loss 1.87784 \n",
      "Epoch 1 | Batch # 78 Train Loss 2.13197 \n",
      "Epoch 1 | Batch # 79 Train Loss 1.74656 \n",
      "Epoch 1 | Batch # 80 Train Loss 2.41986 \n",
      "Epoch 1 | Batch # 81 Train Loss 2.44584 \n",
      "Epoch 1 | Batch # 82 Train Loss 1.62990 \n",
      "Epoch 1 | Batch # 83 Train Loss 1.29226 \n",
      "Epoch 1 | Batch # 84 Train Loss 2.63881 \n",
      "Epoch 1 | Batch # 85 Train Loss 2.52197 \n",
      "Epoch 1 | Batch # 86 Train Loss 2.07590 \n",
      "Epoch 1 | Batch # 87 Train Loss 1.81959 \n",
      "Epoch 1 | Batch # 88 Train Loss 2.47392 \n",
      "Epoch 1 | Batch # 89 Train Loss 3.18389 \n",
      "Epoch 1 | Batch # 90 Train Loss 2.71573 \n",
      "Epoch 1 | Batch # 91 Train Loss 1.80378 \n",
      "Epoch 1 | Batch # 92 Train Loss 2.39021 \n",
      "Epoch 1 | Batch # 93 Train Loss 2.05096 \n",
      "Epoch 1 | Batch # 94 Train Loss 1.40402 \n",
      "Epoch 1 | Batch # 95 Train Loss 2.07226 \n",
      "Epoch 1 | Batch # 96 Train Loss 1.70084 \n",
      "Epoch 1 | Batch # 97 Train Loss 1.17648 \n",
      "Epoch 1 | Batch # 98 Train Loss 2.59980 \n",
      "Epoch 1 | Batch # 99 Train Loss 4.63627 \n",
      "Epoch 1 | Batch # 100 Train Loss 2.22647 \n",
      "Epoch 1 | Batch # 100 Train Loss 2.22647 \n",
      "Epoch 1 | Batch # 101 Train Loss 2.12153 \n",
      "Epoch 1 | Batch # 102 Train Loss 1.67684 \n",
      "Epoch 1 | Batch # 103 Train Loss 1.53863 \n",
      "Epoch 1 | Batch # 104 Train Loss 3.12116 \n",
      "Epoch 1 | Batch # 105 Train Loss 2.03344 \n",
      "Epoch 1 | Batch # 106 Train Loss 2.70166 \n",
      "Epoch 1 | Batch # 107 Train Loss 0.79915 \n",
      "Epoch 1 | Batch # 108 Train Loss 1.78561 \n",
      "Epoch 1 | Batch # 109 Train Loss 1.67894 \n",
      "Epoch 1 | Batch # 110 Train Loss 4.60093 \n",
      "Epoch 1 | Batch # 111 Train Loss 0.67529 \n",
      "Epoch 1 | Batch # 112 Train Loss 2.58972 \n",
      "Epoch 1 | Batch # 113 Train Loss 2.50655 \n",
      "Epoch 1 | Batch # 114 Train Loss 2.15993 \n",
      "Epoch 1 | Batch # 115 Train Loss 3.60082 \n",
      "Epoch 1 | Batch # 116 Train Loss 1.51047 \n",
      "Epoch 1 | Batch # 117 Train Loss 2.27436 \n",
      "Epoch 1 | Batch # 118 Train Loss 1.89991 \n",
      "Epoch 1 | Batch # 119 Train Loss 2.58954 \n",
      "Epoch 1 | Batch # 120 Train Loss 2.66898 \n",
      "Epoch 1 | Batch # 121 Train Loss 1.74918 \n",
      "Epoch 1 | Batch # 122 Train Loss 2.71541 \n",
      "Epoch 1 | Batch # 123 Train Loss 2.86387 \n",
      "Epoch 1 | Batch # 124 Train Loss 2.09190 \n",
      "Epoch 1 | Batch # 125 Train Loss 2.65946 \n",
      "Epoch 1 | Batch # 126 Train Loss 2.81774 \n",
      "Epoch 1 | Batch # 127 Train Loss 1.39142 \n",
      "Epoch 1 | Batch # 128 Train Loss 2.76388 \n",
      "Epoch 1 | Batch # 129 Train Loss 2.52336 \n",
      "Epoch 1 | Batch # 130 Train Loss 2.71991 \n",
      "Epoch 1 | Batch # 131 Train Loss 2.65111 \n",
      "Epoch 1 | Batch # 132 Train Loss 3.29699 \n",
      "Epoch 1 | Batch # 133 Train Loss 1.58908 \n",
      "Epoch 1 | Batch # 134 Train Loss 1.91432 \n",
      "Epoch 1 | Batch # 135 Train Loss 2.15061 \n",
      "Epoch 1 | Batch # 136 Train Loss 1.95265 \n",
      "Epoch 1 | Batch # 137 Train Loss 1.97600 \n",
      "Epoch 1 | Batch # 138 Train Loss 1.37960 \n",
      "Epoch 1 | Batch # 139 Train Loss 1.47929 \n",
      "Epoch 1 | Batch # 140 Train Loss 1.75224 \n",
      "Epoch 1 | Batch # 141 Train Loss 1.40766 \n",
      "Epoch 1 | Batch # 142 Train Loss 1.48721 \n",
      "Epoch 1 | Batch # 143 Train Loss 2.08879 \n",
      "Epoch 1 | Batch # 144 Train Loss 2.74370 \n",
      "Epoch 1 | Batch # 145 Train Loss 3.66263 \n",
      "Epoch 1 | Batch # 146 Train Loss 2.63592 \n",
      "Epoch 1 | Batch # 147 Train Loss 1.15071 \n",
      "Epoch 1 | Batch # 148 Train Loss 1.37222 \n",
      "Epoch 1 | Batch # 149 Train Loss 2.40712 \n",
      "Epoch 1 | Batch # 150 Train Loss 1.70107 \n",
      "Epoch 1 | Batch # 150 Train Loss 1.70107 \n",
      "Epoch 1 | Batch # 151 Train Loss 1.73738 \n",
      "Epoch 1 | Batch # 152 Train Loss 2.79936 \n",
      "Epoch 1 | Batch # 153 Train Loss 1.60275 \n",
      "Epoch 1 | Batch # 154 Train Loss 2.64298 \n",
      "Epoch 1 | Batch # 155 Train Loss 1.71375 \n",
      "Epoch 1 | Batch # 156 Train Loss 2.14329 \n",
      "Epoch 1 | Batch # 157 Train Loss 3.18383 \n",
      "Epoch 1 | Batch # 158 Train Loss 1.51954 \n",
      "Epoch 1 | Batch # 159 Train Loss 0.80184 \n",
      "Epoch 1 | Batch # 160 Train Loss 1.97669 \n",
      "Epoch 1 | Batch # 161 Train Loss 4.42842 \n",
      "Epoch 1 | Batch # 162 Train Loss 2.15699 \n",
      "Epoch 1 | Batch # 163 Train Loss 4.36468 \n",
      "Epoch 1 | Batch # 164 Train Loss 0.85268 \n",
      "Epoch 1 | Batch # 165 Train Loss 2.30445 \n",
      "Epoch 1 | Batch # 166 Train Loss 2.89509 \n",
      "Epoch 1 | Batch # 167 Train Loss 1.90963 \n",
      "Epoch 1 | Batch # 168 Train Loss 2.54126 \n",
      "Epoch 1 | Batch # 169 Train Loss 1.64197 \n",
      "Epoch 1 | Batch # 170 Train Loss 2.16589 \n",
      "Epoch 1 | Batch # 171 Train Loss 4.58979 \n",
      "Epoch 1 | Batch # 172 Train Loss 6.03429 \n",
      "Epoch 1 | Batch # 173 Train Loss 1.56071 \n",
      "Epoch 1 | Batch # 174 Train Loss 3.50064 \n",
      "Epoch 1 | Batch # 175 Train Loss 2.87125 \n",
      "Epoch 1 | Batch # 176 Train Loss 2.68635 \n",
      "Epoch 1 | Batch # 177 Train Loss 2.66031 \n",
      "Epoch 1 | Batch # 178 Train Loss 2.49315 \n",
      "Epoch 1 | Batch # 179 Train Loss 3.17273 \n",
      "Epoch 1 | Batch # 180 Train Loss 3.00200 \n",
      "Epoch 1 | Batch # 181 Train Loss 1.48473 \n",
      "Epoch 1 | Batch # 182 Train Loss 0.88603 \n",
      "Epoch 1 | Batch # 183 Train Loss 0.68862 \n",
      "Epoch 1 | Batch # 184 Train Loss 3.18883 \n",
      "Epoch 1 | Batch # 185 Train Loss 1.18900 \n",
      "Epoch 1 | Batch # 186 Train Loss 2.20656 \n",
      "Epoch 1 | Batch # 187 Train Loss 3.70020 \n",
      "Epoch 1 | Batch # 188 Train Loss 3.96930 \n",
      "Epoch 1 | Batch # 189 Train Loss 2.84324 \n",
      "Epoch 1 | Batch # 190 Train Loss 2.64016 \n",
      "Epoch 1 | Batch # 191 Train Loss 2.17232 \n",
      "Epoch 1 | Batch # 192 Train Loss 2.82323 \n",
      "Epoch 1 | Batch # 193 Train Loss 2.58230 \n",
      "Epoch 1 | Batch # 194 Train Loss 2.86312 \n",
      "Epoch 1 | Batch # 195 Train Loss 2.30893 \n",
      "Epoch 1 | Batch # 196 Train Loss 3.29760 \n",
      "Epoch 1 | Batch # 197 Train Loss 3.76298 \n",
      "Epoch 1 | Batch # 198 Train Loss 3.43842 \n",
      "Epoch 1 | Batch # 199 Train Loss 1.52473 \n",
      "Epoch 1 | Batch # 200 Train Loss 1.84327 \n",
      "Epoch 1 | Batch # 200 Train Loss 1.84327 \n",
      "Epoch 1 | Batch # 201 Train Loss 1.85489 \n",
      "Epoch 1 | Batch # 202 Train Loss 1.51174 \n",
      "Epoch 1 | Batch # 203 Train Loss 2.80764 \n",
      "Epoch 1 | Batch # 204 Train Loss 2.14573 \n",
      "Epoch 1 | Batch # 205 Train Loss 3.48861 \n",
      "Epoch 1 | Batch # 206 Train Loss 2.11674 \n",
      "Epoch 1 | Batch # 207 Train Loss 2.55399 \n",
      "Epoch 1 | Batch # 208 Train Loss 3.54058 \n",
      "Epoch 1 | Batch # 209 Train Loss 2.42399 \n",
      "Epoch 1 | Batch # 210 Train Loss 1.55069 \n",
      "Epoch 1 | Batch # 211 Train Loss 2.19779 \n",
      "Epoch 1 | Batch # 212 Train Loss 1.16414 \n",
      "Epoch 1 | Batch # 213 Train Loss 3.76922 \n",
      "Epoch 1 | Batch # 214 Train Loss 2.50771 \n",
      "Epoch 1 | Batch # 215 Train Loss 4.49787 \n",
      "Epoch 1 | Batch # 216 Train Loss 4.61598 \n",
      "Epoch 1 | Batch # 217 Train Loss 2.25999 \n",
      "Epoch 1 | Batch # 218 Train Loss 1.50743 \n",
      "Epoch 1 | Batch # 219 Train Loss 2.64475 \n",
      "Epoch 1 | Batch # 220 Train Loss 1.28460 \n",
      "Epoch 1 | Batch # 221 Train Loss 2.00229 \n",
      "Epoch 1 | Batch # 222 Train Loss 2.73294 \n",
      "Epoch 1 | Batch # 223 Train Loss 3.68330 \n",
      "Epoch 1 | Batch # 224 Train Loss 3.53929 \n",
      "Epoch 1 | Batch # 225 Train Loss 2.25387 \n",
      "Epoch 1 | Batch # 226 Train Loss 2.61659 \n",
      "Epoch 1 | Batch # 227 Train Loss 2.35348 \n",
      "Epoch 1 | Batch # 228 Train Loss 1.88713 \n",
      "Epoch 1 | Batch # 229 Train Loss 1.09246 \n",
      "Epoch 1 | Batch # 230 Train Loss 1.98308 \n",
      "Epoch 1 | Batch # 231 Train Loss 2.54082 \n",
      "Epoch 1 | Batch # 232 Train Loss 3.44443 \n",
      "Epoch 1 | Batch # 233 Train Loss 3.92633 \n",
      "Epoch 1 | Batch # 234 Train Loss 2.32874 \n",
      "Epoch 1 | Batch # 235 Train Loss 2.15564 \n",
      "Epoch 1 | Batch # 236 Train Loss 1.41513 \n",
      "Epoch 1 | Batch # 237 Train Loss 1.60414 \n",
      "Epoch 1 | Batch # 238 Train Loss 2.63254 \n",
      "Epoch 1 | Batch # 239 Train Loss 1.28189 \n",
      "Epoch 1 | Batch # 240 Train Loss 2.28211 \n",
      "Epoch 1 | Batch # 241 Train Loss 2.39202 \n",
      "Epoch 1 | Batch # 242 Train Loss 2.18267 \n",
      "Epoch 1 | Batch # 243 Train Loss 1.98019 \n",
      "Epoch 1 | Batch # 244 Train Loss 1.57704 \n",
      "Epoch 1 | Batch # 245 Train Loss 1.30409 \n",
      "Epoch 1 | Batch # 246 Train Loss 2.02142 \n",
      "Epoch 1 | Batch # 247 Train Loss 1.23015 \n",
      "Epoch 1 | Batch # 248 Train Loss 4.59150 \n",
      "Epoch 1 | Batch # 249 Train Loss 1.81522 \n",
      "Epoch 1 | Batch # 250 Train Loss 2.52317 \n",
      "Epoch 1 | Batch # 250 Train Loss 2.52317 \n",
      "Epoch 1 | Batch # 251 Train Loss 1.04605 \n",
      "Epoch 1 | Batch # 252 Train Loss 1.92126 \n",
      "Epoch 1 | Batch # 253 Train Loss 2.09384 \n",
      "Epoch 1 | Batch # 254 Train Loss 1.45773 \n",
      "Epoch 1 | Batch # 255 Train Loss 1.39867 \n",
      "Epoch 1 | Batch # 256 Train Loss 1.49745 \n",
      "Epoch 1 | Batch # 257 Train Loss 4.63703 \n",
      "Epoch 1 | Batch # 258 Train Loss 1.51803 \n",
      "Epoch 1 | Batch # 259 Train Loss 1.04125 \n",
      "Epoch 1 | Batch # 260 Train Loss 1.97605 \n",
      "Epoch 1 | Batch # 261 Train Loss 1.97455 \n",
      "Epoch 1 | Batch # 262 Train Loss 2.11996 \n",
      "Epoch 1 | Batch # 263 Train Loss 2.97763 \n",
      "Epoch 1 | Batch # 264 Train Loss 3.59509 \n",
      "Epoch 1 | Batch # 265 Train Loss 1.11356 \n",
      "Epoch 1 | Batch # 266 Train Loss 1.16116 \n",
      "Epoch 1 | Batch # 267 Train Loss 1.65522 \n",
      "Epoch 1 | Batch # 268 Train Loss 1.75700 \n",
      "Epoch 1 | Batch # 269 Train Loss 2.02050 \n",
      "Epoch 1 | Batch # 270 Train Loss 1.18115 \n",
      "Epoch 1 | Batch # 271 Train Loss 1.95184 \n",
      "Epoch 1 | Batch # 272 Train Loss 1.85091 \n",
      "Epoch 1 | Batch # 273 Train Loss 1.27442 \n",
      "Epoch 1 | Batch # 274 Train Loss 2.03925 \n",
      "Epoch 1 | Batch # 275 Train Loss 2.23041 \n",
      "Epoch 1 | Batch # 276 Train Loss 1.79993 \n",
      "Epoch 1 | Batch # 277 Train Loss 2.55636 \n",
      "Epoch 1 | Batch # 278 Train Loss 1.63621 \n",
      "Epoch 1 | Batch # 279 Train Loss 3.25700 \n",
      "Epoch 1 | Batch # 280 Train Loss 1.84700 \n",
      "Epoch 1 | Batch # 281 Train Loss 1.62415 \n",
      "Epoch 1 | Batch # 282 Train Loss 2.42597 \n",
      "Epoch 1 | Batch # 283 Train Loss 2.14347 \n",
      "Epoch 1 | Batch # 284 Train Loss 1.51031 \n",
      "Epoch 1 | Batch # 285 Train Loss 1.62731 \n",
      "Epoch 1 | Batch # 286 Train Loss 1.68172 \n",
      "Epoch 1 | Batch # 287 Train Loss 1.26405 \n",
      "Epoch 1 | Batch # 288 Train Loss 2.11959 \n",
      "Epoch 1 | Batch # 289 Train Loss 2.61412 \n",
      "Epoch 1 | Batch # 290 Train Loss 3.11082 \n",
      "Epoch 1 | Batch # 291 Train Loss 1.76665 \n",
      "Epoch 1 | Batch # 292 Train Loss 2.47162 \n",
      "Epoch 1 | Batch # 293 Train Loss 1.60282 \n",
      "Epoch 1 | Batch # 294 Train Loss 1.96051 \n",
      "Epoch 1 | Batch # 295 Train Loss 2.27275 \n",
      "Epoch 1 | Batch # 296 Train Loss 1.23456 \n",
      "Epoch 1 | Batch # 297 Train Loss 2.35195 \n",
      "Epoch 1 | Batch # 298 Train Loss 1.67617 \n",
      "Epoch 1 | Batch # 299 Train Loss 2.19335 \n",
      "Epoch 1 | Batch # 300 Train Loss 1.32300 \n",
      "Epoch 1 | Batch # 300 Train Loss 1.32300 \n",
      "Epoch 1 | Batch # 301 Train Loss 1.10531 \n",
      "Epoch 1 | Batch # 302 Train Loss 2.00367 \n",
      "Epoch 1 | Batch # 303 Train Loss 1.90246 \n",
      "Epoch 1 | Batch # 304 Train Loss 1.14295 \n",
      "Epoch 1 | Batch # 305 Train Loss 2.45011 \n",
      "Epoch 1 | Batch # 306 Train Loss 2.12295 \n",
      "Epoch 1 | Batch # 307 Train Loss 2.55654 \n",
      "Epoch 1 | Batch # 308 Train Loss 1.57918 \n",
      "Epoch 1 | Batch # 309 Train Loss 1.82967 \n",
      "Epoch 1 | Batch # 310 Train Loss 1.65450 \n",
      "Epoch 1 | Batch # 311 Train Loss 1.54037 \n",
      "Epoch 1 | Batch # 312 Train Loss 1.51338 \n",
      "Epoch 1 | Batch # 313 Train Loss 1.93566 \n",
      "Epoch 1 | Batch # 314 Train Loss 1.25357 \n",
      "Epoch 1 | Batch # 315 Train Loss 1.56362 \n",
      "Epoch 1 | Batch # 316 Train Loss 1.32149 \n",
      "Epoch 1 | Batch # 317 Train Loss 1.63822 \n",
      "Epoch 1 | Batch # 318 Train Loss 1.30512 \n",
      "Epoch 1 | Batch # 319 Train Loss 1.15698 \n",
      "Epoch 1 | Batch # 320 Train Loss 1.27529 \n",
      "Epoch 1 | Batch # 321 Train Loss 1.64502 \n",
      "Epoch 1 | Batch # 322 Train Loss 1.42124 \n",
      "Epoch 1 | Batch # 323 Train Loss 2.85966 \n",
      "Epoch 1 | Batch # 324 Train Loss 0.94000 \n",
      "Epoch 1 | Batch # 325 Train Loss 2.27876 \n",
      "Epoch 1 | Batch # 326 Train Loss 1.14631 \n",
      "Epoch 1 | Batch # 327 Train Loss 2.62200 \n",
      "Epoch 1 | Batch # 328 Train Loss 2.81142 \n",
      "Epoch 1 | Batch # 329 Train Loss 1.83155 \n",
      "Epoch 1 | Batch # 330 Train Loss 0.65820 \n",
      "Epoch 1 | Batch # 331 Train Loss 3.29621 \n",
      "Epoch 1 | Batch # 332 Train Loss 1.41657 \n",
      "Epoch 1 | Batch # 333 Train Loss 1.42223 \n",
      "Epoch 1 | Batch # 334 Train Loss 2.08499 \n",
      "Epoch 1 | Batch # 335 Train Loss 1.41959 \n",
      "Epoch 1 | Batch # 336 Train Loss 1.63056 \n",
      "Epoch 1 | Batch # 337 Train Loss 1.48454 \n",
      "Epoch 1 | Batch # 338 Train Loss 1.34774 \n",
      "Epoch 1 | Batch # 339 Train Loss 1.67795 \n",
      "Epoch 1 | Batch # 340 Train Loss 1.93772 \n",
      "Epoch 1 | Batch # 341 Train Loss 2.77873 \n",
      "Epoch 1 | Batch # 342 Train Loss 1.43666 \n",
      "Epoch 1 | Batch # 343 Train Loss 2.09304 \n",
      "Epoch 1 | Batch # 344 Train Loss 1.72078 \n",
      "Epoch 1 | Batch # 345 Train Loss 1.75365 \n",
      "Epoch 1 | Batch # 346 Train Loss 1.08707 \n",
      "Epoch 1 | Batch # 347 Train Loss 2.20568 \n",
      "Epoch 1 | Batch # 348 Train Loss 1.73096 \n",
      "Epoch 1 | Batch # 349 Train Loss 2.31142 \n",
      "Epoch 1 | Batch # 350 Train Loss 1.51533 \n",
      "Epoch 1 | Batch # 350 Train Loss 1.51533 \n",
      "Epoch 1 | Batch # 351 Train Loss 1.57684 \n",
      "Epoch 1 | Batch # 352 Train Loss 1.68469 \n",
      "Epoch 1 | Batch # 353 Train Loss 1.66228 \n",
      "Epoch 1 | Batch # 354 Train Loss 2.24514 \n",
      "Epoch 1 | Batch # 355 Train Loss 1.31881 \n",
      "Epoch 1 | Batch # 356 Train Loss 1.93110 \n",
      "Epoch 1 | Batch # 357 Train Loss 2.02936 \n",
      "Epoch 1 | Batch # 358 Train Loss 2.61110 \n",
      "Epoch 1 | Batch # 359 Train Loss 2.08623 \n",
      "Epoch 1 | Batch # 360 Train Loss 2.00518 \n",
      "Epoch 1 | Batch # 361 Train Loss 1.92952 \n",
      "Epoch 1 | Batch # 362 Train Loss 1.76212 \n",
      "Epoch 1 | Batch # 363 Train Loss 1.50044 \n",
      "Epoch 1 | Batch # 364 Train Loss 1.32973 \n",
      "Epoch 1 | Batch # 365 Train Loss 1.10577 \n",
      "Epoch 1 | Batch # 366 Train Loss 1.32137 \n",
      "Epoch 1 | Batch # 367 Train Loss 1.72152 \n",
      "Epoch 1 | Batch # 368 Train Loss 1.62725 \n",
      "Epoch 1 | Batch # 369 Train Loss 1.24712 \n",
      "Epoch 1 | Batch # 370 Train Loss 1.49365 \n",
      "Epoch 1 | Batch # 371 Train Loss 2.85768 \n",
      "Epoch 1 | Batch # 372 Train Loss 1.46630 \n",
      "Epoch 1 | Batch # 373 Train Loss 2.40423 \n",
      "Epoch 1 | Batch # 374 Train Loss 2.00044 \n",
      "Epoch 1 | Batch # 375 Train Loss 2.78112 \n",
      "Epoch 1 | Batch # 376 Train Loss 1.50586 \n",
      "Epoch 1 | Batch # 377 Train Loss 2.49967 \n",
      "Epoch 1 | Batch # 378 Train Loss 1.35743 \n",
      "Epoch 1 | Batch # 379 Train Loss 2.14686 \n",
      "Epoch 1 | Batch # 380 Train Loss 1.43554 \n",
      "Epoch 1 | Batch # 381 Train Loss 1.83478 \n",
      "Epoch 1 | Batch # 382 Train Loss 1.31422 \n",
      "Epoch 1 | Batch # 383 Train Loss 1.51258 \n",
      "Epoch 1 | Batch # 384 Train Loss 1.40508 \n",
      "Epoch 1 | Batch # 385 Train Loss 1.75163 \n",
      "Epoch 1 | Batch # 386 Train Loss 1.55574 \n",
      "Epoch 1 | Batch # 387 Train Loss 1.63329 \n",
      "Epoch 1 | Batch # 388 Train Loss 1.11002 \n",
      "Epoch 1 | Batch # 389 Train Loss 1.63062 \n",
      "Epoch 1 | Batch # 390 Train Loss 0.97229 \n",
      "Epoch 1 | Batch # 391 Train Loss 1.07708 \n",
      "Epoch 1 | Batch # 392 Train Loss 1.63335 \n",
      "Epoch 1 | Batch # 393 Train Loss 2.05249 \n",
      "Epoch 1 | Batch # 394 Train Loss 2.00424 \n",
      "Epoch 1 | Batch # 395 Train Loss 2.54438 \n",
      "Epoch 1 | Batch # 396 Train Loss 1.16472 \n",
      "Epoch 1 | Batch # 397 Train Loss 2.89094 \n",
      "Epoch 1 | Batch # 398 Train Loss 2.46217 \n",
      "Epoch 1 | Batch # 399 Train Loss 1.32230 \n",
      "\n",
      "Epoch 1 | Epoch Train Loss 2.32764\n",
      "Epoch 1 | Epoch Train Acc 28.250%\n",
      "Epoch 1 | Epoch Val Loss 1.74973\n",
      "Epoch 1 | Epoch Val Acc 31.000%\n",
      "\n",
      "Epoch 0 | Batch # 0 Train Loss 1.90788 \n",
      "Epoch 0 | Batch # 0 Train Loss 1.90788 \n",
      "Epoch 0 | Batch # 1 Train Loss 53.82599 \n",
      "Epoch 0 | Batch # 2 Train Loss 58.69376 \n",
      "Epoch 0 | Batch # 3 Train Loss 101.53287 \n",
      "Epoch 0 | Batch # 4 Train Loss 67.69681 \n",
      "Epoch 0 | Batch # 5 Train Loss 39.44677 \n",
      "Epoch 0 | Batch # 6 Train Loss 30.65231 \n",
      "Epoch 0 | Batch # 7 Train Loss 57.91868 \n",
      "Epoch 0 | Batch # 8 Train Loss 99.80596 \n",
      "Epoch 0 | Batch # 9 Train Loss 70.93845 \n",
      "Epoch 0 | Batch # 10 Train Loss 52.17497 \n",
      "Epoch 0 | Batch # 11 Train Loss 23.07753 \n",
      "Epoch 0 | Batch # 12 Train Loss 16.41525 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-5:\n",
      "Traceback (most recent call last):\n",
      "  File \"/data/ysw/anaconda3/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/data/ysw/anaconda3/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/data/ysw/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 96, in _worker_loop\n",
      "    r = index_queue.get(timeout=MANAGER_STATUS_CHECK_INTERVAL)\n",
      "  File \"/data/ysw/anaconda3/lib/python3.7/multiprocessing/queues.py\", line 104, in get\n",
      "    if not self._poll(timeout):\n",
      "  File \"/data/ysw/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 257, in poll\n",
      "    return self._poll(timeout)\n",
      "  File \"/data/ysw/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 414, in _poll\n",
      "    r = wait([self], timeout)\n",
      "  File \"/data/ysw/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 920, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/data/ysw/anaconda3/lib/python3.7/selectors.py\", line 415, in select\n",
      "    fd_event_list = self._selector.poll(timeout)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Batch # 13 Train Loss 6.40675 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-46370de43325>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0my_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_hat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_loss_rec = []\n",
    "train_acc_rec = []\n",
    "test_loss_rec = []\n",
    "test_acc_rec = []\n",
    "\n",
    "num_epoch = 5\n",
    "\n",
    "for learning_rate in lr_list:\n",
    "    model = model_pnas(6).to('cuda')\n",
    "    optim = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "    for epoch in range(num_epoch):\n",
    "        epoch_loss = 0\n",
    "        total, correct = 0, 0\n",
    "        for batch_idx, (X_train, y_train) in enumerate(train_loader):\n",
    "            X_train, y_train = X_train.to('cuda'), y_train.to('cuda')\n",
    "            optim.zero_grad()\n",
    "            y_hat = model(X_train)\n",
    "            loss = loss_func(y_hat, torch.max(y_train, 1)[1])\n",
    "            loss.backward()\n",
    "            _, pred = torch.max(y_hat.data, 1)\n",
    "            optim.step()\n",
    "            correct += (pred == torch.max(y_train, 1)[1]).sum().item()\n",
    "            total += X_train.size(0)\n",
    "            epoch_loss += loss.item() / len(train_loader)\n",
    "            if (batch_idx % 50) == 0:\n",
    "                print('LR {} | Epoch {} | Batch # {} Train Loss {:.5f} '.format(learning_rate, epoch, batch_idx, loss.item()))\n",
    "\n",
    "        print('\\nLR {} | Epoch {} | Epoch Train Loss {:.5f}'.format(learning_rate, epoch, epoch_loss))\n",
    "        epoch_acc = correct / total * 100\n",
    "        print('LR {} | Epoch {} | Epoch Train Acc {:.3f}%'.format(learning_rate, epoch, epoch_acc))\n",
    "\n",
    "        with torch.no_grad():\n",
    "            test_epoch_loss = 0\n",
    "            test_total = 0\n",
    "            test_correct = 0\n",
    "\n",
    "            for batch_idx, (X_val, y_val) in enumerate(test_loader):\n",
    "                X_val, y_val = X_val.to('cuda'), y_val.to('cuda')\n",
    "\n",
    "                y_hat = model(X_val)\n",
    "                loss = loss_func(y_hat, torch.max(y_val, 1)[1])\n",
    "                _, pred = torch.max(y_hat.data, 1)\n",
    "                test_total += y_val.size(0)\n",
    "                test_correct += (pred == torch.max(y_val, 1)[1]).sum().item()\n",
    "                test_epoch_loss += loss.item() / len(test_loader)\n",
    "\n",
    "            test_epoch_acc = test_correct / test_total * 100\n",
    "\n",
    "            print('Epoch {} | Epoch Val Loss {:.5f}'.format(epoch, test_epoch_loss))\n",
    "            print('Epoch {} | Epoch Val Acc {:.3f}%\\n'.format(epoch, test_epoch_acc))\n",
    "\n",
    "    test_loss_rec.append(test_epoch_loss)\n",
    "    test_acc_rec.append(test_epoch_acc)\n",
    "\n",
    "    train_loss_rec.append(epoch_loss)\n",
    "    train_acc_rec.append(epoch_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) \n",
    "plt.subplot(2,1,1)\n",
    "plt.plot(train_loss_rec, 'b')\n",
    "plt.plot(test_loss_rec, 'r')\n",
    "\n",
    "plt.title('loss')\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "plt.legend(['Train loss', 'Validation loss'], loc=4)\n",
    "\n",
    "plt.subplot(2,1,2)\n",
    "plt.plot(train_acc_rec, 'b')\n",
    "plt.plot(test_acc_rec, 'r')\n",
    "\n",
    "plt.title('Prediction Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "plt.legend(['Train prediction accuracy' ,'Validation prediction accuracy'], loc=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
