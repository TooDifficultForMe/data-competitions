{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2'\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torchvision import models, transforms\n",
    "import pretrainedmodels\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from data import data_reader\n",
    "\n",
    "Tensor = torch.Tensor\n",
    "rand_m = np.random.random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'xception_gcn'\n",
    "if not os.path.exists('weights/'):\n",
    "    !mkdir weights\n",
    "if not os.path.exists('weights/{}'.format(MODEL_NAME)):\n",
    "    !mkdir weights/{MODEL_NAME}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import model_xception\n",
    "model = model_xception(6, use_gcn=True).to('cuda')\n",
    "optim = torch.optim.Adam(model.parameters())\n",
    "loss_func = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import data_reader\n",
    "class args:\n",
    "    bs = 16\n",
    "    path = 'train2000/'\n",
    "\n",
    "x_norm = transforms.Normalize(mean=[0.5, 0.5, 0.5],\n",
    "                                  std=[0.5, 0.5, 0.5])\n",
    "trm = transforms.Compose([\n",
    "    transforms.Resize(299),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    x_norm\n",
    "])\n",
    "\n",
    "val_trm = transforms.Compose([\n",
    "    transforms.Resize(333),\n",
    "    transforms.CenterCrop(299),\n",
    "    transforms.ToTensor(),\n",
    "    x_norm\n",
    "])\n",
    "\n",
    "dr = data_reader(args, 'train2000/train2000.csv', trm=trm, val_trm=val_trm)\n",
    "\n",
    "train_loader, test_loader = dr.get_train_loader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.train_model import trainer\n",
    "model_trainer = trainer(model, epoch=100, train_loader=train_loader, test_loader=test_loader, \n",
    "        optim=optim, loss_func=loss_func, model_name='xception_gcn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Batch # 0 Train Loss 1.79491 \n",
      "Epoch 0 | Batch # 1 Train Loss 38.97139 \n",
      "Epoch 0 | Batch # 2 Train Loss 1047.56116 \n",
      "Epoch 0 | Batch # 3 Train Loss 756.50763 \n",
      "Epoch 0 | Batch # 4 Train Loss 491.84625 \n",
      "Epoch 0 | Batch # 5 Train Loss 926.57745 \n",
      "Epoch 0 | Batch # 6 Train Loss 717.58691 \n",
      "Epoch 0 | Batch # 7 Train Loss 328.11438 \n",
      "Epoch 0 | Batch # 8 Train Loss 221.27597 \n",
      "Epoch 0 | Batch # 9 Train Loss 618.04797 \n",
      "Epoch 0 | Batch # 10 Train Loss 1269.71045 \n",
      "Epoch 0 | Batch # 11 Train Loss 544.12842 \n",
      "Epoch 0 | Batch # 12 Train Loss 370.44424 \n",
      "Epoch 0 | Batch # 13 Train Loss 485.84854 \n",
      "Epoch 0 | Batch # 14 Train Loss 168.67628 \n",
      "Epoch 0 | Batch # 15 Train Loss 137.81131 \n",
      "Epoch 0 | Batch # 16 Train Loss 98.04649 \n",
      "Epoch 0 | Batch # 17 Train Loss 146.13324 \n",
      "Epoch 0 | Batch # 18 Train Loss 66.00172 \n",
      "Epoch 0 | Batch # 19 Train Loss 97.76830 \n",
      "Epoch 0 | Batch # 20 Train Loss 113.80906 \n",
      "Epoch 0 | Batch # 21 Train Loss 37.65506 \n",
      "Epoch 0 | Batch # 22 Train Loss 127.82623 \n",
      "Epoch 0 | Batch # 23 Train Loss 33.90674 \n",
      "Epoch 0 | Batch # 24 Train Loss 62.99852 \n",
      "Epoch 0 | Batch # 25 Train Loss 163.98692 \n",
      "Epoch 0 | Batch # 26 Train Loss 105.76122 \n",
      "Epoch 0 | Batch # 27 Train Loss 49.68473 \n",
      "Epoch 0 | Batch # 28 Train Loss 10.15994 \n",
      "Epoch 0 | Batch # 29 Train Loss 67.36816 \n",
      "Epoch 0 | Batch # 30 Train Loss 29.98118 \n",
      "Epoch 0 | Batch # 31 Train Loss 78.43714 \n",
      "Epoch 0 | Batch # 32 Train Loss 64.03212 \n",
      "Epoch 0 | Batch # 33 Train Loss 35.65398 \n",
      "Epoch 0 | Batch # 34 Train Loss 26.41517 \n",
      "Epoch 0 | Batch # 35 Train Loss 31.68312 \n",
      "Epoch 0 | Batch # 36 Train Loss 37.93964 \n",
      "Epoch 0 | Batch # 37 Train Loss 76.28108 \n",
      "Epoch 0 | Batch # 38 Train Loss 7.05773 \n",
      "Epoch 0 | Batch # 39 Train Loss 40.74151 \n",
      "Epoch 0 | Batch # 40 Train Loss 25.70488 \n",
      "Epoch 0 | Batch # 41 Train Loss 38.83895 \n",
      "Epoch 0 | Batch # 42 Train Loss 29.82279 \n",
      "Epoch 0 | Batch # 43 Train Loss 25.12828 \n",
      "Epoch 0 | Batch # 44 Train Loss 1.62233 \n",
      "Epoch 0 | Batch # 45 Train Loss 13.12274 \n",
      "Epoch 0 | Batch # 46 Train Loss 14.90064 \n",
      "Epoch 0 | Batch # 47 Train Loss 9.21573 \n",
      "Epoch 0 | Batch # 48 Train Loss 10.10859 \n",
      "Epoch 0 | Batch # 49 Train Loss 19.69142 \n",
      "Epoch 0 | Batch # 50 Train Loss 41.86454 \n",
      "Epoch 0 | Batch # 51 Train Loss 6.38013 \n",
      "Epoch 0 | Batch # 52 Train Loss 67.65577 \n",
      "Epoch 0 | Batch # 53 Train Loss 14.06150 \n",
      "Epoch 0 | Batch # 54 Train Loss 43.23072 \n",
      "Epoch 0 | Batch # 55 Train Loss 36.42190 \n",
      "Epoch 0 | Batch # 56 Train Loss 19.12939 \n",
      "Epoch 0 | Batch # 57 Train Loss 16.90862 \n",
      "Epoch 0 | Batch # 58 Train Loss 102.90502 \n",
      "Epoch 0 | Batch # 59 Train Loss 22.05665 \n",
      "Epoch 0 | Batch # 60 Train Loss 9.12157 \n",
      "Epoch 0 | Batch # 61 Train Loss 12.23646 \n",
      "Epoch 0 | Batch # 62 Train Loss 26.09816 \n",
      "Epoch 0 | Batch # 63 Train Loss 29.98018 \n",
      "Epoch 0 | Batch # 64 Train Loss 24.30465 \n",
      "Epoch 0 | Batch # 65 Train Loss 30.21593 \n",
      "Epoch 0 | Batch # 66 Train Loss 20.04469 \n",
      "Epoch 0 | Batch # 67 Train Loss 2.10361 \n",
      "Epoch 0 | Batch # 68 Train Loss 22.16829 \n",
      "Epoch 0 | Batch # 69 Train Loss 22.33300 \n",
      "Epoch 0 | Batch # 70 Train Loss 44.58938 \n",
      "Epoch 0 | Batch # 71 Train Loss 38.21731 \n",
      "Epoch 0 | Batch # 72 Train Loss 12.31748 \n",
      "Epoch 0 | Batch # 73 Train Loss 18.54863 \n",
      "Epoch 0 | Batch # 74 Train Loss 12.05376 \n",
      "Epoch 0 | Batch # 75 Train Loss 21.05490 \n",
      "Epoch 0 | Batch # 76 Train Loss 17.58122 \n",
      "Epoch 0 | Batch # 77 Train Loss 9.94491 \n",
      "Epoch 0 | Batch # 78 Train Loss 10.07733 \n",
      "Epoch 0 | Batch # 79 Train Loss 11.81386 \n",
      "Epoch 0 | Batch # 80 Train Loss 7.91846 \n",
      "Epoch 0 | Batch # 81 Train Loss 11.88881 \n",
      "Epoch 0 | Batch # 82 Train Loss 39.69928 \n",
      "Epoch 0 | Batch # 83 Train Loss 0.86938 \n",
      "Epoch 0 | Batch # 84 Train Loss 1.92105 \n",
      "Epoch 0 | Batch # 85 Train Loss 2.00262 \n",
      "Epoch 0 | Batch # 86 Train Loss 4.74938 \n",
      "Epoch 0 | Batch # 87 Train Loss 3.30087 \n",
      "Epoch 0 | Batch # 88 Train Loss 6.90840 \n",
      "Epoch 0 | Batch # 89 Train Loss 4.68037 \n",
      "Epoch 0 | Batch # 90 Train Loss 8.06446 \n",
      "Epoch 0 | Batch # 91 Train Loss 18.03451 \n",
      "Epoch 0 | Batch # 92 Train Loss 20.59887 \n",
      "Epoch 0 | Batch # 93 Train Loss 14.53362 \n",
      "Epoch 0 | Batch # 94 Train Loss 16.62077 \n",
      "Epoch 0 | Batch # 95 Train Loss 13.81401 \n",
      "Epoch 0 | Batch # 96 Train Loss 12.49795 \n",
      "Epoch 0 | Batch # 97 Train Loss 7.42420 \n",
      "Epoch 0 | Batch # 98 Train Loss 3.07417 \n",
      "Epoch 0 | Batch # 99 Train Loss 6.91534 \n",
      "Epoch 0 | Epoch Train Loss 108.93325\n",
      "\n",
      "Epoch 0 | Epoch Train Acc 48.812%\n",
      "weight save to weights/xception_gcn/best_params_acc43.0.pth\n",
      "Epoch 0 | Epoch Val Loss 20.86044\n",
      "Epoch 0 | Epoch Val Acc 43.000%\n",
      "Epoch 1 | Batch # 0 Train Loss 13.55384 \n",
      "Epoch 1 | Batch # 1 Train Loss 7.86076 \n",
      "Epoch 1 | Batch # 2 Train Loss 4.42527 \n",
      "Epoch 1 | Batch # 3 Train Loss 3.42211 \n",
      "Epoch 1 | Batch # 4 Train Loss 2.13090 \n",
      "Epoch 1 | Batch # 5 Train Loss 3.37701 \n",
      "Epoch 1 | Batch # 6 Train Loss 3.71328 \n",
      "Epoch 1 | Batch # 7 Train Loss 3.22072 \n",
      "Epoch 1 | Batch # 8 Train Loss 1.43815 \n",
      "Epoch 1 | Batch # 9 Train Loss 3.69217 \n",
      "Epoch 1 | Batch # 10 Train Loss 1.64714 \n",
      "Epoch 1 | Batch # 11 Train Loss 4.38482 \n",
      "Epoch 1 | Batch # 12 Train Loss 3.91005 \n",
      "Epoch 1 | Batch # 13 Train Loss 2.07303 \n",
      "Epoch 1 | Batch # 14 Train Loss 1.78964 \n",
      "Epoch 1 | Batch # 15 Train Loss 1.85862 \n",
      "Epoch 1 | Batch # 16 Train Loss 1.86970 \n",
      "Epoch 1 | Batch # 17 Train Loss 1.38727 \n",
      "Epoch 1 | Batch # 18 Train Loss 2.14683 \n",
      "Epoch 1 | Batch # 19 Train Loss 1.73609 \n",
      "Epoch 1 | Batch # 20 Train Loss 1.92592 \n",
      "Epoch 1 | Batch # 21 Train Loss 0.77530 \n",
      "Epoch 1 | Batch # 22 Train Loss 1.68385 \n",
      "Epoch 1 | Batch # 23 Train Loss 1.59171 \n",
      "Epoch 1 | Batch # 24 Train Loss 3.24390 \n",
      "Epoch 1 | Batch # 25 Train Loss 2.37475 \n",
      "Epoch 1 | Batch # 26 Train Loss 50.30070 \n",
      "Epoch 1 | Batch # 27 Train Loss 1.24400 \n",
      "Epoch 1 | Batch # 28 Train Loss 1.96941 \n",
      "Epoch 1 | Batch # 29 Train Loss 1.99714 \n",
      "Epoch 1 | Batch # 30 Train Loss 1.39920 \n",
      "Epoch 1 | Batch # 31 Train Loss 3.82261 \n",
      "Epoch 1 | Batch # 32 Train Loss 1.47609 \n",
      "Epoch 1 | Batch # 33 Train Loss 9.38216 \n",
      "Epoch 1 | Batch # 34 Train Loss 5.09554 \n",
      "Epoch 1 | Batch # 35 Train Loss 5.25209 \n",
      "Epoch 1 | Batch # 36 Train Loss 7.37250 \n",
      "Epoch 1 | Batch # 37 Train Loss 5.22290 \n",
      "Epoch 1 | Batch # 38 Train Loss 2.38352 \n",
      "Epoch 1 | Batch # 39 Train Loss 3.74733 \n",
      "Epoch 1 | Batch # 40 Train Loss 5.36413 \n",
      "Epoch 1 | Batch # 41 Train Loss 3.57610 \n",
      "Epoch 1 | Batch # 42 Train Loss 0.77157 \n",
      "Epoch 1 | Batch # 43 Train Loss 4.29735 \n",
      "Epoch 1 | Batch # 44 Train Loss 0.70587 \n",
      "Epoch 1 | Batch # 45 Train Loss 0.92438 \n",
      "Epoch 1 | Batch # 46 Train Loss 4.09276 \n",
      "Epoch 1 | Batch # 47 Train Loss 1.86265 \n",
      "Epoch 1 | Batch # 48 Train Loss 2.34701 \n",
      "Epoch 1 | Batch # 49 Train Loss 1.71991 \n",
      "Epoch 1 | Batch # 50 Train Loss 1.32433 \n",
      "Epoch 1 | Batch # 51 Train Loss 0.92688 \n",
      "Epoch 1 | Batch # 52 Train Loss 1.03438 \n",
      "Epoch 1 | Batch # 53 Train Loss 0.40091 \n",
      "Epoch 1 | Batch # 54 Train Loss 1.03711 \n",
      "Epoch 1 | Batch # 55 Train Loss 2.36505 \n",
      "Epoch 1 | Batch # 56 Train Loss 4.00547 \n",
      "Epoch 1 | Batch # 57 Train Loss 1.43508 \n",
      "Epoch 1 | Batch # 58 Train Loss 1.40402 \n",
      "Epoch 1 | Batch # 59 Train Loss 1.31299 \n",
      "Epoch 1 | Batch # 60 Train Loss 1.19017 \n",
      "Epoch 1 | Batch # 61 Train Loss 1.28995 \n",
      "Epoch 1 | Batch # 62 Train Loss 81.59844 \n",
      "Epoch 1 | Batch # 63 Train Loss 0.93600 \n",
      "Epoch 1 | Batch # 64 Train Loss 1.69369 \n",
      "Epoch 1 | Batch # 65 Train Loss 2.83794 \n",
      "Epoch 1 | Batch # 66 Train Loss 4.17481 \n",
      "Epoch 1 | Batch # 67 Train Loss 17.98407 \n",
      "Epoch 1 | Batch # 68 Train Loss 0.69718 \n",
      "Epoch 1 | Batch # 69 Train Loss 5.16212 \n",
      "Epoch 1 | Batch # 70 Train Loss 20.39504 \n",
      "Epoch 1 | Batch # 71 Train Loss 4.13057 \n",
      "Epoch 1 | Batch # 72 Train Loss 0.88334 \n",
      "Epoch 1 | Batch # 73 Train Loss 6.87316 \n",
      "Epoch 1 | Batch # 74 Train Loss 2.16964 \n",
      "Epoch 1 | Batch # 75 Train Loss 3.49829 \n",
      "Epoch 1 | Batch # 76 Train Loss 1.72535 \n",
      "Epoch 1 | Batch # 77 Train Loss 1.91677 \n",
      "Epoch 1 | Batch # 78 Train Loss 6.94275 \n",
      "Epoch 1 | Batch # 79 Train Loss 1.17912 \n",
      "Epoch 1 | Batch # 80 Train Loss 2.92807 \n",
      "Epoch 1 | Batch # 81 Train Loss 2.28382 \n",
      "Epoch 1 | Batch # 82 Train Loss 1.54497 \n",
      "Epoch 1 | Batch # 83 Train Loss 1.51862 \n",
      "Epoch 1 | Batch # 84 Train Loss 5.12739 \n",
      "Epoch 1 | Batch # 85 Train Loss 7.04569 \n",
      "Epoch 1 | Batch # 86 Train Loss 4.46127 \n",
      "Epoch 1 | Batch # 87 Train Loss 4.26701 \n",
      "Epoch 1 | Batch # 88 Train Loss 3.16076 \n",
      "Epoch 1 | Batch # 89 Train Loss 1.48384 \n",
      "Epoch 1 | Batch # 90 Train Loss 1.50954 \n",
      "Epoch 1 | Batch # 91 Train Loss 1.03325 \n",
      "Epoch 1 | Batch # 92 Train Loss 1.08074 \n",
      "Epoch 1 | Batch # 93 Train Loss 1.15120 \n",
      "Epoch 1 | Batch # 94 Train Loss 1.21594 \n",
      "Epoch 1 | Batch # 95 Train Loss 0.92765 \n",
      "Epoch 1 | Batch # 96 Train Loss 0.78195 \n",
      "Epoch 1 | Batch # 97 Train Loss 1.38444 \n",
      "Epoch 1 | Batch # 98 Train Loss 1.33311 \n",
      "Epoch 1 | Batch # 99 Train Loss 1.03670 \n",
      "Epoch 1 | Epoch Train Loss 4.35360\n",
      "\n",
      "Epoch 1 | Epoch Train Acc 51.188%\n",
      "weight save to weights/xception_gcn/best_params_acc63.74999999999999.pth\n",
      "Epoch 1 | Epoch Val Loss 1.80364\n",
      "Epoch 1 | Epoch Val Acc 63.750%\n",
      "Epoch 2 | Batch # 0 Train Loss 0.60673 \n",
      "Epoch 2 | Batch # 1 Train Loss 1.81870 \n",
      "Epoch 2 | Batch # 2 Train Loss 0.89456 \n",
      "Epoch 2 | Batch # 3 Train Loss 1.63057 \n",
      "Epoch 2 | Batch # 4 Train Loss 0.82149 \n",
      "Epoch 2 | Batch # 5 Train Loss 2.03590 \n",
      "Epoch 2 | Batch # 6 Train Loss 1.22406 \n",
      "Epoch 2 | Batch # 7 Train Loss 1.15947 \n",
      "Epoch 2 | Batch # 8 Train Loss 0.83857 \n",
      "Epoch 2 | Batch # 9 Train Loss 1.08293 \n",
      "Epoch 2 | Batch # 10 Train Loss 1.28847 \n",
      "Epoch 2 | Batch # 11 Train Loss 1.86570 \n",
      "Epoch 2 | Batch # 12 Train Loss 0.82235 \n",
      "Epoch 2 | Batch # 13 Train Loss 1.03582 \n",
      "Epoch 2 | Batch # 14 Train Loss 1.62391 \n",
      "Epoch 2 | Batch # 15 Train Loss 0.82387 \n",
      "Epoch 2 | Batch # 16 Train Loss 0.91037 \n",
      "Epoch 2 | Batch # 17 Train Loss 0.96023 \n",
      "Epoch 2 | Batch # 18 Train Loss 1.62521 \n",
      "Epoch 2 | Batch # 19 Train Loss 0.84074 \n",
      "Epoch 2 | Batch # 20 Train Loss 0.85346 \n",
      "Epoch 2 | Batch # 21 Train Loss 1.57764 \n",
      "Epoch 2 | Batch # 22 Train Loss 1.19085 \n",
      "Epoch 2 | Batch # 23 Train Loss 1.63250 \n",
      "Epoch 2 | Batch # 24 Train Loss 1.45064 \n",
      "Epoch 2 | Batch # 25 Train Loss 1.00381 \n",
      "Epoch 2 | Batch # 26 Train Loss 0.52073 \n",
      "Epoch 2 | Batch # 27 Train Loss 0.83587 \n",
      "Epoch 2 | Batch # 28 Train Loss 0.80576 \n",
      "Epoch 2 | Batch # 29 Train Loss 1.10477 \n",
      "Epoch 2 | Batch # 30 Train Loss 0.81737 \n",
      "Epoch 2 | Batch # 31 Train Loss 1.59394 \n",
      "Epoch 2 | Batch # 32 Train Loss 0.57352 \n",
      "Epoch 2 | Batch # 33 Train Loss 1.31390 \n",
      "Epoch 2 | Batch # 34 Train Loss 0.79007 \n",
      "Epoch 2 | Batch # 35 Train Loss 0.64244 \n",
      "Epoch 2 | Batch # 36 Train Loss 1.38428 \n",
      "Epoch 2 | Batch # 37 Train Loss 0.66622 \n",
      "Epoch 2 | Batch # 38 Train Loss 1.12151 \n",
      "Epoch 2 | Batch # 39 Train Loss 1.42552 \n",
      "Epoch 2 | Batch # 40 Train Loss 0.38581 \n",
      "Epoch 2 | Batch # 41 Train Loss 0.73859 \n",
      "Epoch 2 | Batch # 42 Train Loss 0.96216 \n",
      "Epoch 2 | Batch # 43 Train Loss 2.25068 \n",
      "Epoch 2 | Batch # 44 Train Loss 0.54000 \n",
      "Epoch 2 | Batch # 45 Train Loss 0.66590 \n",
      "Epoch 2 | Batch # 46 Train Loss 1.25008 \n",
      "Epoch 2 | Batch # 47 Train Loss 1.33837 \n",
      "Epoch 2 | Batch # 48 Train Loss 0.91361 \n",
      "Epoch 2 | Batch # 49 Train Loss 1.46165 \n",
      "Epoch 2 | Batch # 50 Train Loss 0.64249 \n",
      "Epoch 2 | Batch # 51 Train Loss 0.38649 \n",
      "Epoch 2 | Batch # 52 Train Loss 0.95716 \n",
      "Epoch 2 | Batch # 53 Train Loss 2.16019 \n",
      "Epoch 2 | Batch # 54 Train Loss 0.64100 \n",
      "Epoch 2 | Batch # 55 Train Loss 0.53680 \n",
      "Epoch 2 | Batch # 56 Train Loss 1.21824 \n",
      "Epoch 2 | Batch # 57 Train Loss 1.02295 \n",
      "Epoch 2 | Batch # 58 Train Loss 0.77795 \n",
      "Epoch 2 | Batch # 59 Train Loss 0.89620 \n",
      "Epoch 2 | Batch # 60 Train Loss 1.49192 \n",
      "Epoch 2 | Batch # 61 Train Loss 0.40995 \n",
      "Epoch 2 | Batch # 62 Train Loss 0.72069 \n",
      "Epoch 2 | Batch # 63 Train Loss 0.64681 \n",
      "Epoch 2 | Batch # 64 Train Loss 0.53122 \n",
      "Epoch 2 | Batch # 65 Train Loss 0.95110 \n",
      "Epoch 2 | Batch # 66 Train Loss 0.53569 \n",
      "Epoch 2 | Batch # 67 Train Loss 1.70733 \n",
      "Epoch 2 | Batch # 68 Train Loss 0.86122 \n",
      "Epoch 2 | Batch # 69 Train Loss 0.63727 \n",
      "Epoch 2 | Batch # 70 Train Loss 0.85921 \n",
      "Epoch 2 | Batch # 71 Train Loss 0.84122 \n",
      "Epoch 2 | Batch # 72 Train Loss 0.72712 \n",
      "Epoch 2 | Batch # 73 Train Loss 1.04544 \n",
      "Epoch 2 | Batch # 74 Train Loss 0.46940 \n",
      "Epoch 2 | Batch # 75 Train Loss 1.62538 \n",
      "Epoch 2 | Batch # 76 Train Loss 0.48541 \n",
      "Epoch 2 | Batch # 77 Train Loss 0.99236 \n",
      "Epoch 2 | Batch # 78 Train Loss 0.61024 \n",
      "Epoch 2 | Batch # 79 Train Loss 0.74586 \n",
      "Epoch 2 | Batch # 80 Train Loss 0.51721 \n",
      "Epoch 2 | Batch # 81 Train Loss 0.96374 \n",
      "Epoch 2 | Batch # 82 Train Loss 0.92045 \n",
      "Epoch 2 | Batch # 83 Train Loss 0.71486 \n",
      "Epoch 2 | Batch # 84 Train Loss 0.82284 \n",
      "Epoch 2 | Batch # 85 Train Loss 0.60909 \n",
      "Epoch 2 | Batch # 86 Train Loss 1.74313 \n",
      "Epoch 2 | Batch # 87 Train Loss 0.36749 \n",
      "Epoch 2 | Batch # 88 Train Loss 0.72736 \n",
      "Epoch 2 | Batch # 89 Train Loss 0.89513 \n",
      "Epoch 2 | Batch # 90 Train Loss 0.74868 \n",
      "Epoch 2 | Batch # 91 Train Loss 0.62723 \n",
      "Epoch 2 | Batch # 92 Train Loss 0.74970 \n",
      "Epoch 2 | Batch # 93 Train Loss 0.85724 \n",
      "Epoch 2 | Batch # 94 Train Loss 1.70848 \n",
      "Epoch 2 | Batch # 95 Train Loss 0.71472 \n",
      "Epoch 2 | Batch # 96 Train Loss 0.62572 \n",
      "Epoch 2 | Batch # 97 Train Loss 1.32744 \n",
      "Epoch 2 | Batch # 98 Train Loss 1.65820 \n",
      "Epoch 2 | Batch # 99 Train Loss 0.92769 \n",
      "Epoch 2 | Epoch Train Loss 1.00082\n",
      "\n",
      "Epoch 2 | Epoch Train Acc 67.375%\n",
      "weight save to weights/xception_gcn/best_params_acc69.0.pth\n",
      "Epoch 2 | Epoch Val Loss 1.00442\n",
      "Epoch 2 | Epoch Val Acc 69.000%\n",
      "Epoch 3 | Batch # 0 Train Loss 1.03725 \n",
      "Epoch 3 | Batch # 1 Train Loss 0.78374 \n",
      "Epoch 3 | Batch # 2 Train Loss 0.84867 \n",
      "Epoch 3 | Batch # 3 Train Loss 1.61920 \n",
      "Epoch 3 | Batch # 4 Train Loss 0.67621 \n",
      "Epoch 3 | Batch # 5 Train Loss 1.12697 \n",
      "Epoch 3 | Batch # 6 Train Loss 1.58698 \n",
      "Epoch 3 | Batch # 7 Train Loss 0.60935 \n",
      "Epoch 3 | Batch # 8 Train Loss 0.50474 \n",
      "Epoch 3 | Batch # 9 Train Loss 0.34507 \n",
      "Epoch 3 | Batch # 10 Train Loss 0.78480 \n",
      "Epoch 3 | Batch # 11 Train Loss 2.07722 \n",
      "Epoch 3 | Batch # 12 Train Loss 1.74612 \n",
      "Epoch 3 | Batch # 13 Train Loss 0.36764 \n",
      "Epoch 3 | Batch # 14 Train Loss 1.07757 \n",
      "Epoch 3 | Batch # 15 Train Loss 0.57752 \n",
      "Epoch 3 | Batch # 16 Train Loss 1.46921 \n",
      "Epoch 3 | Batch # 17 Train Loss 2.16166 \n",
      "Epoch 3 | Batch # 18 Train Loss 0.46224 \n",
      "Epoch 3 | Batch # 19 Train Loss 0.49053 \n",
      "Epoch 3 | Batch # 20 Train Loss 1.15087 \n",
      "Epoch 3 | Batch # 21 Train Loss 0.60163 \n",
      "Epoch 3 | Batch # 22 Train Loss 0.65781 \n",
      "Epoch 3 | Batch # 23 Train Loss 0.93869 \n",
      "Epoch 3 | Batch # 24 Train Loss 0.59609 \n",
      "Epoch 3 | Batch # 25 Train Loss 1.19033 \n",
      "Epoch 3 | Batch # 26 Train Loss 0.73907 \n",
      "Epoch 3 | Batch # 27 Train Loss 0.73540 \n",
      "Epoch 3 | Batch # 28 Train Loss 0.46179 \n",
      "Epoch 3 | Batch # 29 Train Loss 1.14116 \n",
      "Epoch 3 | Batch # 30 Train Loss 1.35811 \n",
      "Epoch 3 | Batch # 31 Train Loss 1.49972 \n",
      "Epoch 3 | Batch # 32 Train Loss 1.00342 \n",
      "Epoch 3 | Batch # 33 Train Loss 0.66783 \n",
      "Epoch 3 | Batch # 34 Train Loss 0.46129 \n",
      "Epoch 3 | Batch # 35 Train Loss 0.75672 \n",
      "Epoch 3 | Batch # 36 Train Loss 0.84735 \n",
      "Epoch 3 | Batch # 37 Train Loss 0.93955 \n",
      "Epoch 3 | Batch # 38 Train Loss 0.62146 \n",
      "Epoch 3 | Batch # 39 Train Loss 0.91172 \n",
      "Epoch 3 | Batch # 40 Train Loss 1.34581 \n",
      "Epoch 3 | Batch # 41 Train Loss 0.70572 \n",
      "Epoch 3 | Batch # 42 Train Loss 0.66911 \n",
      "Epoch 3 | Batch # 43 Train Loss 2.37599 \n",
      "Epoch 3 | Batch # 44 Train Loss 0.80280 \n",
      "Epoch 3 | Batch # 45 Train Loss 0.99677 \n",
      "Epoch 3 | Batch # 46 Train Loss 2.10876 \n",
      "Epoch 3 | Batch # 47 Train Loss 1.33735 \n",
      "Epoch 3 | Batch # 48 Train Loss 0.95922 \n",
      "Epoch 3 | Batch # 49 Train Loss 1.26022 \n",
      "Epoch 3 | Batch # 50 Train Loss 0.71237 \n",
      "Epoch 3 | Batch # 51 Train Loss 0.78476 \n",
      "Epoch 3 | Batch # 52 Train Loss 0.77696 \n",
      "Epoch 3 | Batch # 53 Train Loss 1.52791 \n",
      "Epoch 3 | Batch # 54 Train Loss 0.87077 \n",
      "Epoch 3 | Batch # 55 Train Loss 0.99143 \n",
      "Epoch 3 | Batch # 56 Train Loss 0.61309 \n",
      "Epoch 3 | Batch # 57 Train Loss 1.01809 \n",
      "Epoch 3 | Batch # 58 Train Loss 0.54844 \n",
      "Epoch 3 | Batch # 59 Train Loss 1.09015 \n",
      "Epoch 3 | Batch # 60 Train Loss 0.66109 \n",
      "Epoch 3 | Batch # 61 Train Loss 1.33427 \n",
      "Epoch 3 | Batch # 62 Train Loss 0.40604 \n",
      "Epoch 3 | Batch # 63 Train Loss 0.92689 \n",
      "Epoch 3 | Batch # 64 Train Loss 0.57620 \n",
      "Epoch 3 | Batch # 65 Train Loss 0.82797 \n",
      "Epoch 3 | Batch # 66 Train Loss 0.92199 \n",
      "Epoch 3 | Batch # 67 Train Loss 0.97471 \n",
      "Epoch 3 | Batch # 68 Train Loss 0.63971 \n",
      "Epoch 3 | Batch # 69 Train Loss 0.74736 \n",
      "Epoch 3 | Batch # 70 Train Loss 0.78577 \n",
      "Epoch 3 | Batch # 71 Train Loss 0.69134 \n",
      "Epoch 3 | Batch # 72 Train Loss 0.51960 \n",
      "Epoch 3 | Batch # 73 Train Loss 0.75585 \n",
      "Epoch 3 | Batch # 74 Train Loss 0.69757 \n",
      "Epoch 3 | Batch # 75 Train Loss 0.73492 \n",
      "Epoch 3 | Batch # 76 Train Loss 2.04725 \n",
      "Epoch 3 | Batch # 77 Train Loss 1.12761 \n",
      "Epoch 3 | Batch # 78 Train Loss 0.65064 \n",
      "Epoch 3 | Batch # 79 Train Loss 0.71264 \n",
      "Epoch 3 | Batch # 80 Train Loss 0.77236 \n",
      "Epoch 3 | Batch # 81 Train Loss 0.57993 \n",
      "Epoch 3 | Batch # 82 Train Loss 0.75462 \n",
      "Epoch 3 | Batch # 83 Train Loss 1.34844 \n",
      "Epoch 3 | Batch # 84 Train Loss 0.47408 \n",
      "Epoch 3 | Batch # 85 Train Loss 0.39923 \n",
      "Epoch 3 | Batch # 86 Train Loss 0.58220 \n",
      "Epoch 3 | Batch # 87 Train Loss 0.87289 \n",
      "Epoch 3 | Batch # 88 Train Loss 0.62809 \n",
      "Epoch 3 | Batch # 89 Train Loss 0.60708 \n",
      "Epoch 3 | Batch # 90 Train Loss 0.35924 \n",
      "Epoch 3 | Batch # 91 Train Loss 2.77121 \n",
      "Epoch 3 | Batch # 92 Train Loss 3.45746 \n",
      "Epoch 3 | Batch # 93 Train Loss 0.63108 \n",
      "Epoch 3 | Batch # 94 Train Loss 1.26282 \n",
      "Epoch 3 | Batch # 95 Train Loss 0.74287 \n",
      "Epoch 3 | Batch # 96 Train Loss 1.37446 \n",
      "Epoch 3 | Batch # 97 Train Loss 1.00890 \n",
      "Epoch 3 | Batch # 98 Train Loss 0.73307 \n",
      "Epoch 3 | Batch # 99 Train Loss 1.03384 \n",
      "Epoch 3 | Epoch Train Loss 0.96262\n",
      "\n",
      "Epoch 3 | Epoch Train Acc 71.812%\n",
      "weight save to weights/xception_gcn/best_params_acc73.25.pth\n",
      "Epoch 3 | Epoch Val Loss 0.94073\n",
      "Epoch 3 | Epoch Val Acc 73.250%\n",
      "Epoch 4 | Batch # 0 Train Loss 0.73764 \n",
      "Epoch 4 | Batch # 1 Train Loss 1.05036 \n",
      "Epoch 4 | Batch # 2 Train Loss 1.05816 \n",
      "Epoch 4 | Batch # 3 Train Loss 0.82998 \n",
      "Epoch 4 | Batch # 4 Train Loss 1.35290 \n",
      "Epoch 4 | Batch # 5 Train Loss 0.69439 \n",
      "Epoch 4 | Batch # 6 Train Loss 0.74531 \n",
      "Epoch 4 | Batch # 7 Train Loss 0.97180 \n",
      "Epoch 4 | Batch # 8 Train Loss 1.06936 \n",
      "Epoch 4 | Batch # 9 Train Loss 0.96120 \n",
      "Epoch 4 | Batch # 10 Train Loss 1.03704 \n",
      "Epoch 4 | Batch # 11 Train Loss 0.52429 \n",
      "Epoch 4 | Batch # 12 Train Loss 0.31102 \n",
      "Epoch 4 | Batch # 13 Train Loss 0.47354 \n",
      "Epoch 4 | Batch # 14 Train Loss 0.55985 \n",
      "Epoch 4 | Batch # 15 Train Loss 0.82509 \n",
      "Epoch 4 | Batch # 16 Train Loss 0.79636 \n",
      "Epoch 4 | Batch # 17 Train Loss 1.23696 \n",
      "Epoch 4 | Batch # 18 Train Loss 0.79464 \n",
      "Epoch 4 | Batch # 19 Train Loss 1.31586 \n",
      "Epoch 4 | Batch # 20 Train Loss 0.78743 \n",
      "Epoch 4 | Batch # 21 Train Loss 0.66590 \n",
      "Epoch 4 | Batch # 22 Train Loss 0.64311 \n",
      "Epoch 4 | Batch # 23 Train Loss 0.88928 \n",
      "Epoch 4 | Batch # 24 Train Loss 0.73191 \n",
      "Epoch 4 | Batch # 25 Train Loss 0.38707 \n",
      "Epoch 4 | Batch # 26 Train Loss 0.68282 \n",
      "Epoch 4 | Batch # 27 Train Loss 1.09122 \n",
      "Epoch 4 | Batch # 28 Train Loss 0.49425 \n",
      "Epoch 4 | Batch # 29 Train Loss 0.39650 \n",
      "Epoch 4 | Batch # 30 Train Loss 0.59295 \n",
      "Epoch 4 | Batch # 31 Train Loss 0.94896 \n",
      "Epoch 4 | Batch # 32 Train Loss 0.63486 \n",
      "Epoch 4 | Batch # 33 Train Loss 0.70611 \n",
      "Epoch 4 | Batch # 34 Train Loss 0.42212 \n",
      "Epoch 4 | Batch # 35 Train Loss 1.26043 \n",
      "Epoch 4 | Batch # 36 Train Loss 0.41443 \n",
      "Epoch 4 | Batch # 37 Train Loss 1.13516 \n",
      "Epoch 4 | Batch # 38 Train Loss 0.46969 \n",
      "Epoch 4 | Batch # 39 Train Loss 0.77258 \n",
      "Epoch 4 | Batch # 40 Train Loss 0.41211 \n",
      "Epoch 4 | Batch # 41 Train Loss 1.17579 \n",
      "Epoch 4 | Batch # 42 Train Loss 0.50431 \n",
      "Epoch 4 | Batch # 43 Train Loss 0.50423 \n",
      "Epoch 4 | Batch # 44 Train Loss 0.79773 \n",
      "Epoch 4 | Batch # 45 Train Loss 0.91786 \n",
      "Epoch 4 | Batch # 46 Train Loss 0.49986 \n",
      "Epoch 4 | Batch # 47 Train Loss 0.88747 \n",
      "Epoch 4 | Batch # 48 Train Loss 0.66040 \n",
      "Epoch 4 | Batch # 49 Train Loss 0.55971 \n",
      "Epoch 4 | Batch # 50 Train Loss 0.78569 \n",
      "Epoch 4 | Batch # 51 Train Loss 0.77105 \n",
      "Epoch 4 | Batch # 52 Train Loss 0.53162 \n",
      "Epoch 4 | Batch # 53 Train Loss 0.36889 \n",
      "Epoch 4 | Batch # 54 Train Loss 0.73761 \n",
      "Epoch 4 | Batch # 55 Train Loss 0.87741 \n",
      "Epoch 4 | Batch # 56 Train Loss 0.99655 \n",
      "Epoch 4 | Batch # 57 Train Loss 0.69922 \n",
      "Epoch 4 | Batch # 58 Train Loss 0.41712 \n",
      "Epoch 4 | Batch # 59 Train Loss 0.95323 \n",
      "Epoch 4 | Batch # 60 Train Loss 0.81119 \n",
      "Epoch 4 | Batch # 61 Train Loss 0.60833 \n",
      "Epoch 4 | Batch # 62 Train Loss 0.87198 \n",
      "Epoch 4 | Batch # 63 Train Loss 1.18422 \n",
      "Epoch 4 | Batch # 64 Train Loss 0.50307 \n",
      "Epoch 4 | Batch # 65 Train Loss 0.56180 \n",
      "Epoch 4 | Batch # 66 Train Loss 0.89203 \n",
      "Epoch 4 | Batch # 67 Train Loss 0.77819 \n",
      "Epoch 4 | Batch # 68 Train Loss 0.97067 \n",
      "Epoch 4 | Batch # 69 Train Loss 0.57297 \n",
      "Epoch 4 | Batch # 70 Train Loss 0.46746 \n",
      "Epoch 4 | Batch # 71 Train Loss 0.75556 \n",
      "Epoch 4 | Batch # 72 Train Loss 0.65871 \n",
      "Epoch 4 | Batch # 73 Train Loss 0.62274 \n",
      "Epoch 4 | Batch # 74 Train Loss 0.45146 \n",
      "Epoch 4 | Batch # 75 Train Loss 1.25835 \n",
      "Epoch 4 | Batch # 76 Train Loss 0.58359 \n",
      "Epoch 4 | Batch # 77 Train Loss 0.56879 \n",
      "Epoch 4 | Batch # 78 Train Loss 1.66221 \n",
      "Epoch 4 | Batch # 79 Train Loss 0.71583 \n",
      "Epoch 4 | Batch # 80 Train Loss 0.64210 \n",
      "Epoch 4 | Batch # 81 Train Loss 0.77101 \n",
      "Epoch 4 | Batch # 82 Train Loss 0.50494 \n",
      "Epoch 4 | Batch # 83 Train Loss 0.32988 \n",
      "Epoch 4 | Batch # 84 Train Loss 0.44827 \n",
      "Epoch 4 | Batch # 85 Train Loss 1.28297 \n",
      "Epoch 4 | Batch # 86 Train Loss 2.09234 \n",
      "Epoch 4 | Batch # 87 Train Loss 0.70860 \n",
      "Epoch 4 | Batch # 88 Train Loss 0.71699 \n",
      "Epoch 4 | Batch # 89 Train Loss 0.83204 \n",
      "Epoch 4 | Batch # 90 Train Loss 0.84189 \n",
      "Epoch 4 | Batch # 91 Train Loss 0.71698 \n",
      "Epoch 4 | Batch # 92 Train Loss 0.88571 \n",
      "Epoch 4 | Batch # 93 Train Loss 0.49947 \n",
      "Epoch 4 | Batch # 94 Train Loss 0.69557 \n",
      "Epoch 4 | Batch # 95 Train Loss 0.65915 \n",
      "Epoch 4 | Batch # 96 Train Loss 1.33791 \n",
      "Epoch 4 | Batch # 97 Train Loss 0.75968 \n",
      "Epoch 4 | Batch # 98 Train Loss 0.86910 \n",
      "Epoch 4 | Batch # 99 Train Loss 0.64719 \n",
      "Epoch 4 | Epoch Train Loss 0.77271\n",
      "\n",
      "Epoch 4 | Epoch Train Acc 76.562%\n",
      "weight save to weights/xception_gcn/best_params_acc74.5.pth\n",
      "Epoch 4 | Epoch Val Loss 0.71822\n",
      "Epoch 4 | Epoch Val Acc 74.500%\n",
      "Epoch 5 | Batch # 0 Train Loss 0.74413 \n",
      "Epoch 5 | Batch # 1 Train Loss 0.56848 \n",
      "Epoch 5 | Batch # 2 Train Loss 0.79900 \n",
      "Epoch 5 | Batch # 3 Train Loss 0.43017 \n",
      "Epoch 5 | Batch # 4 Train Loss 0.79337 \n",
      "Epoch 5 | Batch # 5 Train Loss 0.97531 \n",
      "Epoch 5 | Batch # 6 Train Loss 2.46512 \n",
      "Epoch 5 | Batch # 7 Train Loss 0.56436 \n",
      "Epoch 5 | Batch # 8 Train Loss 0.55063 \n",
      "Epoch 5 | Batch # 9 Train Loss 0.40637 \n",
      "Epoch 5 | Batch # 10 Train Loss 1.52569 \n",
      "Epoch 5 | Batch # 11 Train Loss 0.64433 \n",
      "Epoch 5 | Batch # 12 Train Loss 0.99448 \n",
      "Epoch 5 | Batch # 13 Train Loss 0.49758 \n",
      "Epoch 5 | Batch # 14 Train Loss 0.73617 \n",
      "Epoch 5 | Batch # 15 Train Loss 0.70159 \n",
      "Epoch 5 | Batch # 16 Train Loss 0.50754 \n",
      "Epoch 5 | Batch # 17 Train Loss 0.42864 \n",
      "Epoch 5 | Batch # 18 Train Loss 0.38311 \n",
      "Epoch 5 | Batch # 19 Train Loss 1.32856 \n",
      "Epoch 5 | Batch # 20 Train Loss 0.54245 \n",
      "Epoch 5 | Batch # 21 Train Loss 0.64611 \n",
      "Epoch 5 | Batch # 22 Train Loss 0.94718 \n",
      "Epoch 5 | Batch # 23 Train Loss 1.34263 \n",
      "Epoch 5 | Batch # 24 Train Loss 0.93755 \n",
      "Epoch 5 | Batch # 25 Train Loss 0.31012 \n",
      "Epoch 5 | Batch # 26 Train Loss 0.34192 \n",
      "Epoch 5 | Batch # 27 Train Loss 0.60242 \n",
      "Epoch 5 | Batch # 28 Train Loss 0.40679 \n",
      "Epoch 5 | Batch # 29 Train Loss 0.58094 \n",
      "Epoch 5 | Batch # 30 Train Loss 0.66187 \n",
      "Epoch 5 | Batch # 31 Train Loss 0.45305 \n",
      "Epoch 5 | Batch # 32 Train Loss 0.52185 \n",
      "Epoch 5 | Batch # 33 Train Loss 0.61007 \n",
      "Epoch 5 | Batch # 34 Train Loss 0.51754 \n",
      "Epoch 5 | Batch # 35 Train Loss 0.29914 \n",
      "Epoch 5 | Batch # 36 Train Loss 0.71214 \n",
      "Epoch 5 | Batch # 37 Train Loss 0.49517 \n",
      "Epoch 5 | Batch # 38 Train Loss 0.74788 \n",
      "Epoch 5 | Batch # 39 Train Loss 1.34339 \n",
      "Epoch 5 | Batch # 40 Train Loss 0.95004 \n",
      "Epoch 5 | Batch # 41 Train Loss 1.79251 \n",
      "Epoch 5 | Batch # 42 Train Loss 1.04532 \n",
      "Epoch 5 | Batch # 43 Train Loss 1.03228 \n",
      "Epoch 5 | Batch # 44 Train Loss 0.88456 \n",
      "Epoch 5 | Batch # 45 Train Loss 1.34662 \n",
      "Epoch 5 | Batch # 46 Train Loss 1.62025 \n",
      "Epoch 5 | Batch # 47 Train Loss 0.51218 \n",
      "Epoch 5 | Batch # 48 Train Loss 1.04482 \n",
      "Epoch 5 | Batch # 49 Train Loss 0.74093 \n",
      "Epoch 5 | Batch # 50 Train Loss 0.93298 \n",
      "Epoch 5 | Batch # 51 Train Loss 0.51163 \n",
      "Epoch 5 | Batch # 52 Train Loss 0.60948 \n",
      "Epoch 5 | Batch # 53 Train Loss 0.71483 \n",
      "Epoch 5 | Batch # 54 Train Loss 0.65065 \n",
      "Epoch 5 | Batch # 55 Train Loss 0.87151 \n",
      "Epoch 5 | Batch # 56 Train Loss 1.24180 \n",
      "Epoch 5 | Batch # 57 Train Loss 0.90158 \n",
      "Epoch 5 | Batch # 58 Train Loss 0.49841 \n",
      "Epoch 5 | Batch # 59 Train Loss 0.75573 \n",
      "Epoch 5 | Batch # 60 Train Loss 0.32491 \n",
      "Epoch 5 | Batch # 61 Train Loss 0.74041 \n",
      "Epoch 5 | Batch # 62 Train Loss 1.07773 \n",
      "Epoch 5 | Batch # 63 Train Loss 0.61841 \n",
      "Epoch 5 | Batch # 64 Train Loss 1.71130 \n",
      "Epoch 5 | Batch # 65 Train Loss 0.67003 \n",
      "Epoch 5 | Batch # 66 Train Loss 0.77428 \n",
      "Epoch 5 | Batch # 67 Train Loss 1.47748 \n",
      "Epoch 5 | Batch # 68 Train Loss 0.69085 \n",
      "Epoch 5 | Batch # 69 Train Loss 0.82903 \n",
      "Epoch 5 | Batch # 70 Train Loss 0.58794 \n",
      "Epoch 5 | Batch # 71 Train Loss 1.02405 \n",
      "Epoch 5 | Batch # 72 Train Loss 0.63635 \n",
      "Epoch 5 | Batch # 73 Train Loss 1.19940 \n",
      "Epoch 5 | Batch # 74 Train Loss 0.91373 \n",
      "Epoch 5 | Batch # 75 Train Loss 0.77764 \n",
      "Epoch 5 | Batch # 76 Train Loss 0.46304 \n",
      "Epoch 5 | Batch # 77 Train Loss 0.80680 \n",
      "Epoch 5 | Batch # 78 Train Loss 0.88884 \n",
      "Epoch 5 | Batch # 79 Train Loss 1.09271 \n",
      "Epoch 5 | Batch # 80 Train Loss 0.55749 \n",
      "Epoch 5 | Batch # 81 Train Loss 0.62318 \n",
      "Epoch 5 | Batch # 82 Train Loss 0.81367 \n",
      "Epoch 5 | Batch # 83 Train Loss 0.57217 \n",
      "Epoch 5 | Batch # 84 Train Loss 0.59389 \n",
      "Epoch 5 | Batch # 85 Train Loss 0.34031 \n",
      "Epoch 5 | Batch # 86 Train Loss 0.63782 \n",
      "Epoch 5 | Batch # 87 Train Loss 0.47041 \n",
      "Epoch 5 | Batch # 88 Train Loss 0.30721 \n",
      "Epoch 5 | Batch # 89 Train Loss 0.57781 \n",
      "Epoch 5 | Batch # 90 Train Loss 1.31345 \n",
      "Epoch 5 | Batch # 91 Train Loss 0.31182 \n",
      "Epoch 5 | Batch # 92 Train Loss 0.63430 \n",
      "Epoch 5 | Batch # 93 Train Loss 1.08458 \n",
      "Epoch 5 | Batch # 94 Train Loss 0.30932 \n",
      "Epoch 5 | Batch # 95 Train Loss 0.78068 \n",
      "Epoch 5 | Batch # 96 Train Loss 0.48717 \n",
      "Epoch 5 | Batch # 97 Train Loss 0.61358 \n",
      "Epoch 5 | Batch # 98 Train Loss 0.86293 \n",
      "Epoch 5 | Batch # 99 Train Loss 0.35366 \n",
      "Epoch 5 | Epoch Train Loss 0.77273\n",
      "\n",
      "Epoch 5 | Epoch Train Acc 75.312%\n",
      "weight save to weights/xception_gcn/best_params_acc78.0.pth\n",
      "Epoch 5 | Epoch Val Loss 0.68629\n",
      "Epoch 5 | Epoch Val Acc 78.000%\n",
      "Epoch 6 | Batch # 0 Train Loss 0.47626 \n",
      "Epoch 6 | Batch # 1 Train Loss 0.53755 \n",
      "Epoch 6 | Batch # 2 Train Loss 0.70367 \n",
      "Epoch 6 | Batch # 3 Train Loss 0.38355 \n",
      "Epoch 6 | Batch # 4 Train Loss 0.62699 \n",
      "Epoch 6 | Batch # 5 Train Loss 0.84508 \n",
      "Epoch 6 | Batch # 6 Train Loss 0.30663 \n",
      "Epoch 6 | Batch # 7 Train Loss 1.36517 \n",
      "Epoch 6 | Batch # 8 Train Loss 0.14446 \n",
      "Epoch 6 | Batch # 9 Train Loss 0.78541 \n",
      "Epoch 6 | Batch # 10 Train Loss 0.66744 \n",
      "Epoch 6 | Batch # 11 Train Loss 0.83712 \n",
      "Epoch 6 | Batch # 12 Train Loss 0.42756 \n",
      "Epoch 6 | Batch # 13 Train Loss 0.93944 \n",
      "Epoch 6 | Batch # 14 Train Loss 0.38213 \n",
      "Epoch 6 | Batch # 15 Train Loss 0.64703 \n",
      "Epoch 6 | Batch # 16 Train Loss 0.65772 \n",
      "Epoch 6 | Batch # 17 Train Loss 0.61861 \n",
      "Epoch 6 | Batch # 18 Train Loss 0.25873 \n",
      "Epoch 6 | Batch # 19 Train Loss 0.70313 \n",
      "Epoch 6 | Batch # 20 Train Loss 0.44526 \n",
      "Epoch 6 | Batch # 21 Train Loss 0.66585 \n",
      "Epoch 6 | Batch # 22 Train Loss 0.56267 \n",
      "Epoch 6 | Batch # 23 Train Loss 0.33457 \n",
      "Epoch 6 | Batch # 24 Train Loss 0.24193 \n",
      "Epoch 6 | Batch # 25 Train Loss 0.78734 \n",
      "Epoch 6 | Batch # 26 Train Loss 1.85132 \n",
      "Epoch 6 | Batch # 27 Train Loss 0.28359 \n",
      "Epoch 6 | Batch # 28 Train Loss 0.98115 \n",
      "Epoch 6 | Batch # 29 Train Loss 0.70424 \n",
      "Epoch 6 | Batch # 30 Train Loss 0.72883 \n",
      "Epoch 6 | Batch # 31 Train Loss 0.42653 \n",
      "Epoch 6 | Batch # 32 Train Loss 0.70670 \n",
      "Epoch 6 | Batch # 33 Train Loss 0.72465 \n",
      "Epoch 6 | Batch # 34 Train Loss 0.52705 \n",
      "Epoch 6 | Batch # 35 Train Loss 0.66436 \n",
      "Epoch 6 | Batch # 36 Train Loss 1.88575 \n",
      "Epoch 6 | Batch # 37 Train Loss 0.39445 \n",
      "Epoch 6 | Batch # 38 Train Loss 0.69193 \n",
      "Epoch 6 | Batch # 39 Train Loss 0.90680 \n",
      "Epoch 6 | Batch # 40 Train Loss 0.43324 \n",
      "Epoch 6 | Batch # 41 Train Loss 0.53688 \n",
      "Epoch 6 | Batch # 42 Train Loss 0.95859 \n",
      "Epoch 6 | Batch # 43 Train Loss 1.48401 \n",
      "Epoch 6 | Batch # 44 Train Loss 0.53434 \n",
      "Epoch 6 | Batch # 45 Train Loss 0.85394 \n",
      "Epoch 6 | Batch # 46 Train Loss 0.94939 \n",
      "Epoch 6 | Batch # 47 Train Loss 1.56384 \n",
      "Epoch 6 | Batch # 48 Train Loss 0.66937 \n",
      "Epoch 6 | Batch # 49 Train Loss 0.74742 \n",
      "Epoch 6 | Batch # 50 Train Loss 0.81638 \n",
      "Epoch 6 | Batch # 51 Train Loss 0.56714 \n",
      "Epoch 6 | Batch # 52 Train Loss 0.28251 \n",
      "Epoch 6 | Batch # 53 Train Loss 0.88299 \n",
      "Epoch 6 | Batch # 54 Train Loss 0.09615 \n",
      "Epoch 6 | Batch # 55 Train Loss 1.54668 \n",
      "Epoch 6 | Batch # 56 Train Loss 1.51718 \n",
      "Epoch 6 | Batch # 57 Train Loss 0.48413 \n",
      "Epoch 6 | Batch # 58 Train Loss 0.98412 \n",
      "Epoch 6 | Batch # 59 Train Loss 1.67735 \n",
      "Epoch 6 | Batch # 60 Train Loss 0.68242 \n",
      "Epoch 6 | Batch # 61 Train Loss 0.39340 \n",
      "Epoch 6 | Batch # 62 Train Loss 0.21236 \n",
      "Epoch 6 | Batch # 63 Train Loss 1.17735 \n",
      "Epoch 6 | Batch # 64 Train Loss 0.44750 \n",
      "Epoch 6 | Batch # 65 Train Loss 1.87035 \n",
      "Epoch 6 | Batch # 66 Train Loss 0.90671 \n",
      "Epoch 6 | Batch # 67 Train Loss 0.33620 \n",
      "Epoch 6 | Batch # 68 Train Loss 0.82780 \n",
      "Epoch 6 | Batch # 69 Train Loss 1.01709 \n",
      "Epoch 6 | Batch # 70 Train Loss 0.53311 \n",
      "Epoch 6 | Batch # 71 Train Loss 0.37526 \n",
      "Epoch 6 | Batch # 72 Train Loss 1.25721 \n",
      "Epoch 6 | Batch # 73 Train Loss 1.05514 \n",
      "Epoch 6 | Batch # 74 Train Loss 0.99572 \n",
      "Epoch 6 | Batch # 75 Train Loss 0.67031 \n",
      "Epoch 6 | Batch # 76 Train Loss 0.31857 \n",
      "Epoch 6 | Batch # 77 Train Loss 0.87551 \n",
      "Epoch 6 | Batch # 78 Train Loss 0.51316 \n",
      "Epoch 6 | Batch # 79 Train Loss 0.32448 \n",
      "Epoch 6 | Batch # 80 Train Loss 0.46643 \n",
      "Epoch 6 | Batch # 81 Train Loss 0.99163 \n",
      "Epoch 6 | Batch # 82 Train Loss 0.86820 \n",
      "Epoch 6 | Batch # 83 Train Loss 0.88055 \n",
      "Epoch 6 | Batch # 84 Train Loss 0.73179 \n",
      "Epoch 6 | Batch # 85 Train Loss 0.88136 \n",
      "Epoch 6 | Batch # 86 Train Loss 1.31498 \n",
      "Epoch 6 | Batch # 87 Train Loss 1.25796 \n",
      "Epoch 6 | Batch # 88 Train Loss 0.71904 \n",
      "Epoch 6 | Batch # 89 Train Loss 0.99815 \n",
      "Epoch 6 | Batch # 90 Train Loss 0.65215 \n",
      "Epoch 6 | Batch # 91 Train Loss 0.37803 \n",
      "Epoch 6 | Batch # 92 Train Loss 0.59860 \n",
      "Epoch 6 | Batch # 93 Train Loss 0.37593 \n",
      "Epoch 6 | Batch # 94 Train Loss 0.50869 \n",
      "Epoch 6 | Batch # 95 Train Loss 0.43695 \n",
      "Epoch 6 | Batch # 96 Train Loss 0.32854 \n",
      "Epoch 6 | Batch # 97 Train Loss 0.75447 \n",
      "Epoch 6 | Batch # 98 Train Loss 0.53889 \n",
      "Epoch 6 | Batch # 99 Train Loss 0.62123 \n",
      "Epoch 6 | Epoch Train Loss 0.73508\n",
      "\n",
      "Epoch 6 | Epoch Train Acc 76.625%\n",
      "Epoch 6 | Epoch Val Loss 0.77676\n",
      "Epoch 6 | Epoch Val Acc 74.750%\n",
      "Epoch 7 | Batch # 0 Train Loss 0.61303 \n",
      "Epoch 7 | Batch # 1 Train Loss 0.74066 \n",
      "Epoch 7 | Batch # 2 Train Loss 1.35755 \n",
      "Epoch 7 | Batch # 3 Train Loss 1.03242 \n",
      "Epoch 7 | Batch # 4 Train Loss 0.91228 \n",
      "Epoch 7 | Batch # 5 Train Loss 0.76848 \n",
      "Epoch 7 | Batch # 6 Train Loss 0.44210 \n",
      "Epoch 7 | Batch # 7 Train Loss 0.40591 \n",
      "Epoch 7 | Batch # 8 Train Loss 0.42173 \n",
      "Epoch 7 | Batch # 9 Train Loss 0.86069 \n",
      "Epoch 7 | Batch # 10 Train Loss 0.69785 \n",
      "Epoch 7 | Batch # 11 Train Loss 0.63307 \n",
      "Epoch 7 | Batch # 12 Train Loss 0.40704 \n",
      "Epoch 7 | Batch # 13 Train Loss 0.42529 \n",
      "Epoch 7 | Batch # 14 Train Loss 0.75208 \n",
      "Epoch 7 | Batch # 15 Train Loss 0.87370 \n",
      "Epoch 7 | Batch # 16 Train Loss 0.35954 \n",
      "Epoch 7 | Batch # 17 Train Loss 0.78703 \n",
      "Epoch 7 | Batch # 18 Train Loss 0.43251 \n",
      "Epoch 7 | Batch # 19 Train Loss 0.69567 \n",
      "Epoch 7 | Batch # 20 Train Loss 0.61334 \n",
      "Epoch 7 | Batch # 21 Train Loss 0.85165 \n",
      "Epoch 7 | Batch # 22 Train Loss 0.33645 \n",
      "Epoch 7 | Batch # 23 Train Loss 0.33031 \n",
      "Epoch 7 | Batch # 24 Train Loss 1.04705 \n",
      "Epoch 7 | Batch # 25 Train Loss 0.60428 \n",
      "Epoch 7 | Batch # 26 Train Loss 0.56052 \n",
      "Epoch 7 | Batch # 27 Train Loss 1.72592 \n",
      "Epoch 7 | Batch # 28 Train Loss 0.62456 \n",
      "Epoch 7 | Batch # 29 Train Loss 1.43067 \n",
      "Epoch 7 | Batch # 30 Train Loss 0.77632 \n",
      "Epoch 7 | Batch # 31 Train Loss 0.44094 \n",
      "Epoch 7 | Batch # 32 Train Loss 0.72976 \n",
      "Epoch 7 | Batch # 33 Train Loss 0.52685 \n",
      "Epoch 7 | Batch # 34 Train Loss 0.71375 \n",
      "Epoch 7 | Batch # 35 Train Loss 0.83461 \n",
      "Epoch 7 | Batch # 36 Train Loss 0.74691 \n",
      "Epoch 7 | Batch # 37 Train Loss 0.36482 \n",
      "Epoch 7 | Batch # 38 Train Loss 0.54102 \n",
      "Epoch 7 | Batch # 39 Train Loss 0.58342 \n",
      "Epoch 7 | Batch # 40 Train Loss 0.89290 \n",
      "Epoch 7 | Batch # 41 Train Loss 0.95443 \n",
      "Epoch 7 | Batch # 42 Train Loss 0.50843 \n",
      "Epoch 7 | Batch # 43 Train Loss 0.76759 \n",
      "Epoch 7 | Batch # 44 Train Loss 0.47973 \n",
      "Epoch 7 | Batch # 45 Train Loss 1.67074 \n",
      "Epoch 7 | Batch # 46 Train Loss 0.94899 \n",
      "Epoch 7 | Batch # 47 Train Loss 0.35129 \n",
      "Epoch 7 | Batch # 48 Train Loss 0.66738 \n",
      "Epoch 7 | Batch # 49 Train Loss 1.67307 \n",
      "Epoch 7 | Batch # 50 Train Loss 0.73687 \n",
      "Epoch 7 | Batch # 51 Train Loss 0.81801 \n",
      "Epoch 7 | Batch # 52 Train Loss 0.60869 \n",
      "Epoch 7 | Batch # 53 Train Loss 0.67283 \n",
      "Epoch 7 | Batch # 54 Train Loss 0.64711 \n",
      "Epoch 7 | Batch # 55 Train Loss 0.73621 \n",
      "Epoch 7 | Batch # 56 Train Loss 0.99170 \n",
      "Epoch 7 | Batch # 57 Train Loss 0.63091 \n",
      "Epoch 7 | Batch # 58 Train Loss 0.69822 \n",
      "Epoch 7 | Batch # 59 Train Loss 0.65655 \n",
      "Epoch 7 | Batch # 60 Train Loss 0.37576 \n",
      "Epoch 7 | Batch # 61 Train Loss 0.43473 \n",
      "Epoch 7 | Batch # 62 Train Loss 0.46804 \n",
      "Epoch 7 | Batch # 63 Train Loss 0.77612 \n",
      "Epoch 7 | Batch # 64 Train Loss 0.41979 \n",
      "Epoch 7 | Batch # 65 Train Loss 0.60765 \n",
      "Epoch 7 | Batch # 66 Train Loss 1.22514 \n",
      "Epoch 7 | Batch # 67 Train Loss 0.80532 \n",
      "Epoch 7 | Batch # 68 Train Loss 1.02839 \n",
      "Epoch 7 | Batch # 69 Train Loss 0.39245 \n",
      "Epoch 7 | Batch # 70 Train Loss 0.51737 \n",
      "Epoch 7 | Batch # 71 Train Loss 0.51601 \n",
      "Epoch 7 | Batch # 72 Train Loss 0.60328 \n",
      "Epoch 7 | Batch # 73 Train Loss 0.48294 \n",
      "Epoch 7 | Batch # 74 Train Loss 1.00766 \n",
      "Epoch 7 | Batch # 75 Train Loss 2.42660 \n",
      "Epoch 7 | Batch # 76 Train Loss 0.58130 \n",
      "Epoch 7 | Batch # 77 Train Loss 0.58552 \n",
      "Epoch 7 | Batch # 78 Train Loss 0.48359 \n",
      "Epoch 7 | Batch # 79 Train Loss 0.80255 \n",
      "Epoch 7 | Batch # 80 Train Loss 0.66721 \n",
      "Epoch 7 | Batch # 81 Train Loss 0.69936 \n",
      "Epoch 7 | Batch # 82 Train Loss 0.40172 \n",
      "Epoch 7 | Batch # 83 Train Loss 0.59611 \n",
      "Epoch 7 | Batch # 84 Train Loss 0.78450 \n",
      "Epoch 7 | Batch # 85 Train Loss 0.58149 \n",
      "Epoch 7 | Batch # 86 Train Loss 0.41515 \n",
      "Epoch 7 | Batch # 87 Train Loss 0.96971 \n",
      "Epoch 7 | Batch # 88 Train Loss 0.85831 \n",
      "Epoch 7 | Batch # 89 Train Loss 0.13269 \n",
      "Epoch 7 | Batch # 90 Train Loss 3.14550 \n",
      "Epoch 7 | Batch # 91 Train Loss 0.47079 \n",
      "Epoch 7 | Batch # 92 Train Loss 0.71076 \n",
      "Epoch 7 | Batch # 93 Train Loss 1.00232 \n",
      "Epoch 7 | Batch # 94 Train Loss 0.65795 \n",
      "Epoch 7 | Batch # 95 Train Loss 0.56432 \n",
      "Epoch 7 | Batch # 96 Train Loss 1.27379 \n",
      "Epoch 7 | Batch # 97 Train Loss 0.62801 \n",
      "Epoch 7 | Batch # 98 Train Loss 0.56901 \n",
      "Epoch 7 | Batch # 99 Train Loss 0.66493 \n",
      "Epoch 7 | Epoch Train Loss 0.74447\n",
      "\n",
      "Epoch 7 | Epoch Train Acc 77.000%\n",
      "Epoch 7 | Epoch Val Loss 1.15532\n",
      "Epoch 7 | Epoch Val Acc 58.000%\n",
      "Epoch 8 | Batch # 0 Train Loss 0.74963 \n",
      "Epoch 8 | Batch # 1 Train Loss 0.99885 \n",
      "Epoch 8 | Batch # 2 Train Loss 0.73685 \n",
      "Epoch 8 | Batch # 3 Train Loss 0.37181 \n",
      "Epoch 8 | Batch # 4 Train Loss 0.61735 \n",
      "Epoch 8 | Batch # 5 Train Loss 0.90104 \n",
      "Epoch 8 | Batch # 6 Train Loss 0.92573 \n",
      "Epoch 8 | Batch # 7 Train Loss 0.25573 \n",
      "Epoch 8 | Batch # 8 Train Loss 0.36747 \n",
      "Epoch 8 | Batch # 9 Train Loss 0.31480 \n",
      "Epoch 8 | Batch # 10 Train Loss 0.33035 \n",
      "Epoch 8 | Batch # 11 Train Loss 0.36624 \n",
      "Epoch 8 | Batch # 12 Train Loss 1.18978 \n",
      "Epoch 8 | Batch # 13 Train Loss 0.23037 \n",
      "Epoch 8 | Batch # 14 Train Loss 0.44879 \n",
      "Epoch 8 | Batch # 15 Train Loss 0.37258 \n",
      "Epoch 8 | Batch # 16 Train Loss 0.57679 \n",
      "Epoch 8 | Batch # 17 Train Loss 0.75320 \n",
      "Epoch 8 | Batch # 18 Train Loss 0.62136 \n",
      "Epoch 8 | Batch # 19 Train Loss 1.02910 \n",
      "Epoch 8 | Batch # 20 Train Loss 1.11228 \n",
      "Epoch 8 | Batch # 21 Train Loss 1.03502 \n",
      "Epoch 8 | Batch # 22 Train Loss 0.72588 \n",
      "Epoch 8 | Batch # 23 Train Loss 0.69179 \n",
      "Epoch 8 | Batch # 24 Train Loss 0.54325 \n",
      "Epoch 8 | Batch # 25 Train Loss 1.50242 \n",
      "Epoch 8 | Batch # 26 Train Loss 0.38608 \n",
      "Epoch 8 | Batch # 27 Train Loss 0.52688 \n",
      "Epoch 8 | Batch # 28 Train Loss 0.44545 \n",
      "Epoch 8 | Batch # 29 Train Loss 0.65708 \n",
      "Epoch 8 | Batch # 30 Train Loss 0.44322 \n",
      "Epoch 8 | Batch # 31 Train Loss 0.63489 \n",
      "Epoch 8 | Batch # 32 Train Loss 0.62767 \n",
      "Epoch 8 | Batch # 33 Train Loss 0.56896 \n",
      "Epoch 8 | Batch # 34 Train Loss 0.55919 \n",
      "Epoch 8 | Batch # 35 Train Loss 0.31220 \n",
      "Epoch 8 | Batch # 36 Train Loss 0.78386 \n",
      "Epoch 8 | Batch # 37 Train Loss 0.44310 \n",
      "Epoch 8 | Batch # 38 Train Loss 0.38155 \n",
      "Epoch 8 | Batch # 39 Train Loss 0.24070 \n",
      "Epoch 8 | Batch # 40 Train Loss 1.10665 \n",
      "Epoch 8 | Batch # 41 Train Loss 0.52055 \n",
      "Epoch 8 | Batch # 42 Train Loss 0.71865 \n",
      "Epoch 8 | Batch # 43 Train Loss 0.33052 \n",
      "Epoch 8 | Batch # 44 Train Loss 1.19031 \n",
      "Epoch 8 | Batch # 45 Train Loss 0.38075 \n",
      "Epoch 8 | Batch # 46 Train Loss 0.63997 \n",
      "Epoch 8 | Batch # 47 Train Loss 0.57421 \n",
      "Epoch 8 | Batch # 48 Train Loss 0.50797 \n",
      "Epoch 8 | Batch # 49 Train Loss 0.50090 \n",
      "Epoch 8 | Batch # 50 Train Loss 1.05832 \n",
      "Epoch 8 | Batch # 51 Train Loss 0.86421 \n",
      "Epoch 8 | Batch # 52 Train Loss 0.82409 \n",
      "Epoch 8 | Batch # 53 Train Loss 0.84517 \n",
      "Epoch 8 | Batch # 54 Train Loss 0.18215 \n",
      "Epoch 8 | Batch # 55 Train Loss 0.83592 \n",
      "Epoch 8 | Batch # 56 Train Loss 0.54997 \n",
      "Epoch 8 | Batch # 57 Train Loss 0.93943 \n",
      "Epoch 8 | Batch # 58 Train Loss 0.53124 \n",
      "Epoch 8 | Batch # 59 Train Loss 0.63783 \n",
      "Epoch 8 | Batch # 60 Train Loss 0.85535 \n",
      "Epoch 8 | Batch # 61 Train Loss 0.53126 \n",
      "Epoch 8 | Batch # 62 Train Loss 0.67007 \n",
      "Epoch 8 | Batch # 63 Train Loss 0.80467 \n",
      "Epoch 8 | Batch # 64 Train Loss 0.29733 \n",
      "Epoch 8 | Batch # 65 Train Loss 0.32565 \n",
      "Epoch 8 | Batch # 66 Train Loss 0.84029 \n",
      "Epoch 8 | Batch # 67 Train Loss 0.42473 \n",
      "Epoch 8 | Batch # 68 Train Loss 0.28605 \n",
      "Epoch 8 | Batch # 69 Train Loss 0.36319 \n",
      "Epoch 8 | Batch # 70 Train Loss 0.17701 \n",
      "Epoch 8 | Batch # 71 Train Loss 0.78253 \n",
      "Epoch 8 | Batch # 72 Train Loss 0.55397 \n",
      "Epoch 8 | Batch # 73 Train Loss 0.49981 \n",
      "Epoch 8 | Batch # 74 Train Loss 1.20158 \n",
      "Epoch 8 | Batch # 75 Train Loss 0.73921 \n",
      "Epoch 8 | Batch # 76 Train Loss 0.36941 \n",
      "Epoch 8 | Batch # 77 Train Loss 0.67976 \n",
      "Epoch 8 | Batch # 78 Train Loss 0.56038 \n",
      "Epoch 8 | Batch # 79 Train Loss 0.25046 \n",
      "Epoch 8 | Batch # 80 Train Loss 0.64334 \n",
      "Epoch 8 | Batch # 81 Train Loss 0.19945 \n",
      "Epoch 8 | Batch # 82 Train Loss 0.89947 \n",
      "Epoch 8 | Batch # 83 Train Loss 0.54352 \n",
      "Epoch 8 | Batch # 84 Train Loss 0.25720 \n",
      "Epoch 8 | Batch # 85 Train Loss 0.31348 \n",
      "Epoch 8 | Batch # 86 Train Loss 0.43540 \n",
      "Epoch 8 | Batch # 87 Train Loss 0.45037 \n",
      "Epoch 8 | Batch # 88 Train Loss 0.88488 \n",
      "Epoch 8 | Batch # 89 Train Loss 0.77188 \n",
      "Epoch 8 | Batch # 90 Train Loss 0.91620 \n",
      "Epoch 8 | Batch # 91 Train Loss 0.45176 \n",
      "Epoch 8 | Batch # 92 Train Loss 0.27122 \n",
      "Epoch 8 | Batch # 93 Train Loss 0.31252 \n",
      "Epoch 8 | Batch # 94 Train Loss 2.15625 \n",
      "Epoch 8 | Batch # 95 Train Loss 0.37663 \n",
      "Epoch 8 | Batch # 96 Train Loss 0.80482 \n",
      "Epoch 8 | Batch # 97 Train Loss 0.24893 \n",
      "Epoch 8 | Batch # 98 Train Loss 0.79134 \n",
      "Epoch 8 | Batch # 99 Train Loss 0.84349 \n",
      "Epoch 8 | Epoch Train Loss 0.62303\n",
      "\n",
      "Epoch 8 | Epoch Train Acc 79.688%\n",
      "Epoch 8 | Epoch Val Loss 0.77711\n",
      "Epoch 8 | Epoch Val Acc 76.250%\n",
      "Epoch 9 | Batch # 0 Train Loss 0.64369 \n",
      "Epoch 9 | Batch # 1 Train Loss 0.23949 \n",
      "Epoch 9 | Batch # 2 Train Loss 0.60985 \n",
      "Epoch 9 | Batch # 3 Train Loss 0.68467 \n",
      "Epoch 9 | Batch # 4 Train Loss 0.57116 \n",
      "Epoch 9 | Batch # 5 Train Loss 0.65460 \n",
      "Epoch 9 | Batch # 6 Train Loss 0.23188 \n",
      "Epoch 9 | Batch # 7 Train Loss 0.23811 \n",
      "Epoch 9 | Batch # 8 Train Loss 0.26698 \n",
      "Epoch 9 | Batch # 9 Train Loss 0.25290 \n",
      "Epoch 9 | Batch # 10 Train Loss 0.87688 \n",
      "Epoch 9 | Batch # 11 Train Loss 0.19540 \n",
      "Epoch 9 | Batch # 12 Train Loss 0.10946 \n",
      "Epoch 9 | Batch # 13 Train Loss 0.19625 \n",
      "Epoch 9 | Batch # 14 Train Loss 1.05289 \n",
      "Epoch 9 | Batch # 15 Train Loss 0.45007 \n",
      "Epoch 9 | Batch # 16 Train Loss 0.16386 \n",
      "Epoch 9 | Batch # 17 Train Loss 0.24049 \n",
      "Epoch 9 | Batch # 18 Train Loss 0.08258 \n",
      "Epoch 9 | Batch # 19 Train Loss 0.43637 \n",
      "Epoch 9 | Batch # 20 Train Loss 0.70223 \n",
      "Epoch 9 | Batch # 21 Train Loss 0.24814 \n",
      "Epoch 9 | Batch # 22 Train Loss 0.35863 \n",
      "Epoch 9 | Batch # 23 Train Loss 1.21916 \n",
      "Epoch 9 | Batch # 24 Train Loss 0.18151 \n",
      "Epoch 9 | Batch # 25 Train Loss 0.64520 \n",
      "Epoch 9 | Batch # 26 Train Loss 1.32732 \n",
      "Epoch 9 | Batch # 27 Train Loss 0.89151 \n",
      "Epoch 9 | Batch # 28 Train Loss 0.28541 \n",
      "Epoch 9 | Batch # 29 Train Loss 0.28183 \n",
      "Epoch 9 | Batch # 30 Train Loss 2.33798 \n",
      "Epoch 9 | Batch # 31 Train Loss 0.45615 \n",
      "Epoch 9 | Batch # 32 Train Loss 0.35576 \n",
      "Epoch 9 | Batch # 33 Train Loss 1.06927 \n",
      "Epoch 9 | Batch # 34 Train Loss 0.91142 \n",
      "Epoch 9 | Batch # 35 Train Loss 0.44888 \n",
      "Epoch 9 | Batch # 36 Train Loss 1.11928 \n",
      "Epoch 9 | Batch # 37 Train Loss 0.69062 \n",
      "Epoch 9 | Batch # 38 Train Loss 0.76446 \n",
      "Epoch 9 | Batch # 39 Train Loss 0.87414 \n",
      "Epoch 9 | Batch # 40 Train Loss 0.48197 \n",
      "Epoch 9 | Batch # 41 Train Loss 0.96239 \n",
      "Epoch 9 | Batch # 42 Train Loss 0.76640 \n",
      "Epoch 9 | Batch # 43 Train Loss 0.61909 \n",
      "Epoch 9 | Batch # 44 Train Loss 0.46252 \n",
      "Epoch 9 | Batch # 45 Train Loss 0.33162 \n",
      "Epoch 9 | Batch # 46 Train Loss 0.90170 \n"
     ]
    }
   ],
   "source": [
    "model_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
